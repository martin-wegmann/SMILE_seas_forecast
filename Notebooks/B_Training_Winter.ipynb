{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training winter forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Martin Wegmann\n",
    "\n",
    "Email: martin.wegmann@unibe.ch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to train the monthly prediction of the DJF seasons T2M and SLP from November SSTs. \n",
    "The files that are read in are already normalized. Check the preprocessing notebook for that. \n",
    "We try out a couple of different architectures, including simple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this notebook is very non-automaized. Time was not a big constraint in my work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example how to do it for the winter season. The exact same structure is done for the summer season, meaning May SSTs as input for JJA T2M and SLP predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 17:15:05.459684: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow.keras.backend as K\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from tensorflow.keras import models\n",
    "from collections import OrderedDict\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, SimpleRNN, LSTM, GRU, Reshape\n",
    "from keras import initializers\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras import Model, Input, regularizers\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, UpSampling2D, Add, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing import image\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazypredict.Supervised import LazyRegressor\n",
    "\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_method = tf.image.ResizeMethod.NEAREST_NEIGHBOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_check_txt(text=\"check\"):\n",
    "    with open(text+'.txt', 'w') as f:\n",
    "        f.write(text)\n",
    "    return print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loss_plot(history,folder,name=\"example\"):\n",
    "    fig = plt.figure(figsize=(8, 4), dpi= 200)\n",
    "    loss =history.history[\"loss\"]\n",
    "    val_loss =history.history[\"val_loss\"]\n",
    "    epochs=range(1,len(loss)+1)\n",
    "\n",
    "    print(np.min(val_loss))\n",
    "\n",
    "\n",
    "    print(np.min(loss))\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(epochs,loss,\"b\",color=\"blue\",label=\"Training loss\")\n",
    "    plt.plot(epochs,val_loss,\"b\",color=\"red\",label=\"Validation loss\")\n",
    "    plt.title(\"Loss Curves for \"+name)\n",
    "    plt.title(\"VL: \"+str(round(np.min(val_loss),2)),loc=\"left\")\n",
    "    plt.title(\"TL: \"+str(round(np.min(loss),2)),loc=\"right\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.savefig(folder+\"Loss Curves for \"+name+\".png\")\n",
    "    plt.show()\n",
    "    return print(\"saved in \"+folder+\"Loss Curves for \"+name+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define target season and variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we generate 5 members for each target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=\"t2m_DJF\"# target_list=[\"t2m_dec\",\"t2m_jan\",\"t2m_feb\",\"t2m_DJF\",\"slp_DJF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_model_member=\"01\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder=\"/storage/homefs/mawegmann/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readin_mpige=\"/storage/homefs/mawegmann/data/mpi_ge/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readin_data=\"/storage/homefs/mawegmann/data/mpi_ge/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read in normalized, masked SSTs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kind of data sets do we have in here?\n",
    "\n",
    "* MPI-GE Control\n",
    "\n",
    "* MPI-GE Hist\n",
    "\n",
    "* MPI-GE RCP26\n",
    "\n",
    "* CODA SSTs 1850-1899 for training\n",
    "\n",
    "* CODA SSTs 1900-2015 for testing\n",
    "\n",
    "We focus on November mean SSTs here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All training datasets are normalized with the same mean and standard deviation (coming from Mpi-GE).\n",
    "The test data is normalized by the mean and standard deviation of the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mpi ge ssts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetao_nov = xr.open_dataset(readin_mpige+\"thetao7_OImon_traintest_nov_lowlow_supermasked_anomnorm.nc\")\n",
    "thetao_nov = thetao_nov.thetao.drop('lev')\n",
    "thetao_nov= thetao_nov.squeeze('lev')\n",
    "\n",
    "thetao_nov_valid = xr.open_dataset(readin_mpige+\"thetao7_OImon_validate_nov_lowlow_supermasked_anomnorm.nc\")\n",
    "thetao_nov_valid = thetao_nov_valid.thetao.drop('lev')\n",
    "thetao_nov_valid= thetao_nov_valid.squeeze('lev')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetao_nov_ctl = xr.open_dataset(readin_mpige+\"thetao_control_nov_lowlow_supermasked_anomnorm.nc\")\n",
    "thetao_nov_ctl = thetao_nov_ctl.thetao.drop('lev')\n",
    "thetao_nov_ctl= thetao_nov_ctl.squeeze('lev')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetao_nov_rcp26 = xr.open_dataset(readin_mpige+\"thetao7_OImon_traintest_nov_lowlow_supermasked_rcp26_anomnorm.nc\")\n",
    "thetao_nov_rcp26 = thetao_nov_rcp26.thetao.drop('lev')\n",
    "thetao_nov_rcp26= thetao_nov_rcp26.squeeze('lev')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coda ssts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetao_nov_codaearly = xr.open_dataset(readin_data+\"sst.mon.mean_18501899_nov_lowlow_supermasked_anomnorm.nc\")\n",
    "thetao_nov_codaearly = thetao_nov_codaearly.sst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetao_nov_codalate = xr.open_dataset(readin_data+\"sst.mon.mean_19002014_nov_lowlow_supermasked_anomnorm.nc\")\n",
    "thetao_nov_codalate = thetao_nov_codalate.sst\n",
    "\n",
    "thetao_nov_codalate_refwindow=thetao_nov_codalate[51:81,:,:]\n",
    "thetao_nov_codalate_climate=thetao_nov_codalate_refwindow.mean(\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in normalized Atmospheric Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kind of data sets do we have in here?\n",
    "\n",
    "* MPI-GE Control\n",
    "\n",
    "* MPI-GE Hist\n",
    "\n",
    "* MPI-GE RCP26\n",
    "\n",
    "* 20CRv3 ensmean 1850-1899 for training\n",
    "\n",
    "* 20Crv3 ensmean 1900-2015 for testing\n",
    "\n",
    "We focus on December, January and February fields here as well as the DJF mean. \n",
    "\n",
    "We just look at 2m temperature and Sea Level Pressure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All training datasets are normalized with the same mean and standard deviation (coming from Mpi-GE).\n",
    "The test data is normalized by the mean and standard deviation of the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason the SST data has only 99 members in MPI-GE instead of 100, so we have to cut our atmospheric data a bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mpi ge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_DJF = xr.open_dataset(readin_mpige+\"tas_Amon_MPI-ESM_historical_all_i1850p3_185001-200512_traintest_DJF_lowlow_anomnorm.nc\")\n",
    "t2m_DJF =t2m_DJF.tas\n",
    "t2m_DJF = t2m_DJF[:15190,:,:]\n",
    "\n",
    "lons=t2m_DJF.lon\n",
    "lats=t2m_DJF.lat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_dec= xr.open_dataset(readin_mpige+\"tas_Amon_MPI-ESM_historical_all_i1850p3_185001-200512_traintest_dec_lowlow_anomnorm.nc\")\n",
    "t2m_dec =t2m_dec.tas\n",
    "t2m_dec = t2m_dec[:15190,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_jan = xr.open_dataset(readin_mpige+\"tas_Amon_MPI-ESM_historical_all_i1850p3_185001-200512_traintest_jan_lowlow_anomnorm.nc\")\n",
    "t2m_jan =t2m_jan.tas\n",
    "t2m_jan = t2m_jan[:15190,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_feb = xr.open_dataset(readin_mpige+\"tas_Amon_MPI-ESM_historical_all_i1850p3_185001-200512_traintest_feb_lowlow_anomnorm.nc\")\n",
    "t2m_feb =t2m_feb.tas\n",
    "t2m_feb = t2m_feb[:15190,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slp_DJF = xr.open_dataset(readin_mpige+\"psl_Amon_MPI-ESM_historical_all_i1850p3_185001-200512_traintest_DJF_lowlow_anomnorm.nc\")\n",
    "slp_DJF =slp_DJF.psl\n",
    "slp_DJF = slp_DJF[:15190,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_DJF_ctl = xr.open_dataset(readin_mpige+\"tas_Amon_mpige_control_DJF_lowlow_anomnorm.nc\")\n",
    "t2m_DJF_ctl =t2m_DJF_ctl.tas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_dec_ctl = xr.open_dataset(readin_mpige+\"tas_Amon_mpige_control_dec_lowlow_anomnorm.nc\")\n",
    "t2m_dec_ctl =t2m_dec_ctl.tas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t2m_jan_ctl = xr.open_dataset(readin_mpige+\"tas_Amon_mpige_control_jan_lowlow_anomnorm.nc\")\n",
    "t2m_jan_ctl =t2m_jan_ctl.tas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_feb_ctl = xr.open_dataset(readin_mpige+\"tas_Amon_mpige_control_feb_lowlow_anomnorm.nc\")\n",
    "t2m_feb_ctl =t2m_feb_ctl.tas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slp_DJF_ctl = xr.open_dataset(readin_mpige+\"psl_Amon_mpige_control_DJF_lowlow_anomnorm.nc\")\n",
    "slp_DJF_ctl =slp_DJF_ctl.psl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_DJF_rcp26 = xr.open_dataset(readin_mpige+\"tas_Amon_MPI-ESM_rcp26_all_i2005p3_200601-209912_traintest_DJF_lowlow_anomnorm.nc\")\n",
    "t2m_DJF_rcp26  =t2m_DJF_rcp26.tas\n",
    "\n",
    "t2m_DJF_rcp26= t2m_DJF_rcp26[:9114,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_dec_rcp26 = xr.open_dataset(readin_mpige+\"tas_Amon_MPI-ESM_rcp26_all_i2005p3_200601-209912_traintest_dec_lowlow_anomnorm.nc\")\n",
    "t2m_dec_rcp26  =t2m_dec_rcp26.tas\n",
    "\n",
    "t2m_dec_rcp26= t2m_dec_rcp26[:9114,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_jan_rcp26 = xr.open_dataset(readin_mpige+\"tas_Amon_MPI-ESM_rcp26_all_i2005p3_200601-209912_traintest_jan_lowlow_anomnorm.nc\")\n",
    "t2m_jan_rcp26  =t2m_jan_rcp26.tas\n",
    "t2m_jan_rcp26= t2m_jan_rcp26[:9114,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_feb_rcp26 = xr.open_dataset(readin_mpige+\"tas_Amon_MPI-ESM_rcp26_all_i2005p3_200601-209912_traintest_feb_lowlow_anomnorm.nc\")\n",
    "t2m_feb_rcp26  =t2m_feb_rcp26.tas\n",
    "t2m_feb_rcp26= t2m_feb_rcp26[:9114,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slp_DJF_rcp26 = xr.open_dataset(readin_mpige+\"psl_Amon_MPI-ESM_rcp26_all_i2005p3_200601-209912_traintest_DJF_lowlow_anomnorm.nc\")\n",
    "slp_DJF_rcp26  =slp_DJF_rcp26.psl\n",
    "slp_DJF_rcp26= slp_DJF_rcp26[:9114,:,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20CRv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_DJF_codaearly = xr.open_dataset(readin_data+\"air.2m.mon.mean_DJF_18511900_lowlow_anomnorm.nc\")\n",
    "t2m_DJF_codaearly  =t2m_DJF_codaearly.air\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slp_DJF_codaearly = xr.open_dataset(readin_data+\"prmsl.mon.mean_DJF_18511900_lowlow_anomnorm.nc\")\n",
    "slp_DJF_codaearly  =slp_DJF_codaearly.prmsl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_dec_codaearly = xr.open_dataset(readin_data+\"air.2m.mon.mean_18501899_dec_lowlow_anomnorm.nc\")\n",
    "t2m_dec_codaearly  =t2m_dec_codaearly.air\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_jan_codaearly = xr.open_dataset(readin_data+\"air.2m.mon.mean_18511900_jan_lowlow_anomnorm.nc\")\n",
    "t2m_jan_codaearly  =t2m_jan_codaearly.air\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_feb_codaearly = xr.open_dataset(readin_data+\"air.2m.mon.mean_18511900_feb_lowlow_anomnorm.nc\")\n",
    "t2m_feb_codaearly  =t2m_feb_codaearly.air\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_DJF_codalate = xr.open_dataset(readin_data+\"air.2m.mon.mean_DJF_19012015_lowlow_anomnorm.nc\")\n",
    "t2m_DJF_codalate  =t2m_DJF_codalate.air\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slp_DJF_codalate = xr.open_dataset(readin_data+\"prmsl.mon.mean_DJF_19012015_lowlow_anomnorm.nc\")\n",
    "slp_DJF_codalate  =slp_DJF_codalate.prmsl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_dec_codalate = xr.open_dataset(readin_data+\"air.2m.mon.mean_19002014_dec_lowlow_anomnorm.nc\")\n",
    "t2m_dec_codalate  =t2m_dec_codalate.air\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_jan_codalate = xr.open_dataset(readin_data+\"air.2m.mon.mean_19012015_jan_lowlow_anomnorm.nc\")\n",
    "t2m_jan_codalate  =t2m_jan_codalate.air\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_feb_codalate = xr.open_dataset(readin_data+\"air.2m.mon.mean_19012015_feb_lowlow_anomnorm.nc\")\n",
    "t2m_feb_codalate  =t2m_feb_codalate.air\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare fields for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we can stack the data to multiple channels or postprocess in any form we need to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_hist_nov=thetao_nov.reset_index(\"time\",drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_ctl_nov=thetao_nov_ctl.reset_index(\"time\",drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_rcp26_nov=thetao_nov_rcp26.reset_index(\"time\",drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_hist_valid_nov=thetao_nov_valid.reset_index(\"time\",drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_coda_20th_nov=thetao_nov_codalate.reset_index(\"time\",drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_coda_19th_nov=thetao_nov_codaearly.reset_index(\"time\",drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create empty containers to stack (not needed here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That way you can stack multiple channels (such as sep, oct, nov ssts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_stacked_hist = []\n",
    "sst_stacked_ctl = []\n",
    "sst_stacked_rcp26 = []\n",
    "sst_stacked_coda_20th= []\n",
    "sst_stacked_coda_19th= []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_mpi.append(ds_mpi)\n",
    "#data_mpi.append(dst_mpi)\n",
    "sst_stacked_hist.append(sst_hist_nov)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_ctl.append(ds_ctl)\n",
    "#data_ctl.append(dst_ctl)\n",
    "sst_stacked_ctl.append(sst_ctl_nov)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_rcp26.append(ds_rcp26)\n",
    "#data_rcp26.append(dst_rcp26)\n",
    "sst_stacked_rcp26.append(sst_rcp26_nov)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_coda.append(ds_test_coda)\n",
    "#data_coda.append(dst_test_coda)\n",
    "sst_stacked_coda_20th.append(sst_coda_20th_nov)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_codaearly.append(ds_codaearly)\n",
    "#data_codaearly.append(dst_codaearly)\n",
    "sst_stacked_coda_19th.append(sst_coda_19th_nov)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_stacked_hist = xr.concat(sst_stacked_hist, 'level').transpose('time', 'lat', 'lon', 'level')\n",
    "sst_stacked_ctl = xr.concat(sst_stacked_ctl, 'level').transpose('time', 'lat', 'lon', 'level')\n",
    "sst_stacked_rcp26 = xr.concat(sst_stacked_rcp26, 'level').transpose('time', 'lat', 'lon', 'level')\n",
    "sst_stacked_coda_20th = xr.concat(sst_stacked_coda_20th, 'level').transpose('time', 'lat', 'lon', 'level')\n",
    "sst_stacked_coda_19th= xr.concat(sst_stacked_coda_19th, 'level').transpose('time', 'lat', 'lon', 'level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if target==\"t2m_DJF\":\n",
    "    target_hist=t2m_DJF\n",
    "    target_coda_20th=t2m_DJF_codalate\n",
    "\n",
    "    target_ctl=t2m_DJF_ctl\n",
    "    target_rcp26=t2m_DJF_rcp26\n",
    "    target_coda_19th=t2m_DJF_codaearly\n",
    "\n",
    "if target==\"t2m_dec\":\n",
    "    target_hist=t2m_dec\n",
    "    target_coda_20th=t2m_dec_codalate\n",
    "\n",
    "    target_ctl=t2m_dec_ctl\n",
    "    target_rcp26=t2m_dec_rcp26\n",
    "    target_coda_19th=t2m_dec_codaearly\n",
    "\n",
    "\n",
    "if target==\"t2m_jan\":\n",
    "    target_hist=t2m_jan\n",
    "    target_coda_20th=t2m_jan_codalate\n",
    "\n",
    "    target_ctl=t2m_jan_ctl\n",
    "    target_rcp26=t2m_jan_rcp26\n",
    "    target_coda_19th=t2m_jan_codaearly\n",
    "\n",
    "if target==\"t2m_feb\":\n",
    "    target_hist=t2m_feb\n",
    "    target_coda_20th=t2m_feb_codalate\n",
    "\n",
    "    target_ctl=t2m_feb_ctl\n",
    "    target_rcp26=t2m_feb_rcp26\n",
    "    target_coda_19th=t2m_feb_codaearly\n",
    "\n",
    "\n",
    "if target==\"slp_DJF\":\n",
    "    target_hist=slp_DJF\n",
    "    target_coda_20th=slp_DJF_codalate\n",
    "\n",
    "    target_ctl=slp_DJF_ctl\n",
    "    target_rcp26=slp_DJF_rcp26\n",
    "    target_coda_19th=slp_DJF_codaearly\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert NA fields into zeroes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_stacked_hist=sst_stacked_hist.fillna(0).values\n",
    "sst_stacked_coda_20th=sst_stacked_coda_20th.fillna(0).values\n",
    "\n",
    "\n",
    "sst_stacked_coda_19th=sst_stacked_coda_19th.fillna(0).values\n",
    "sst_stacked_ctl=sst_stacked_ctl.fillna(0).values\n",
    "sst_stacked_rcp26=sst_stacked_rcp26.fillna(0).values\n",
    "\n",
    "\n",
    "target_hist=target_hist.fillna(0).values\n",
    "target_coda_20th=target_coda_20th.fillna(0).values\n",
    "target_coda_19th=target_coda_19th.fillna(0).values\n",
    "target_ctl=target_ctl.fillna(0).values\n",
    "target_rcp26=target_rcp26.fillna(0).values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define X values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_hist_ctl_rcp26=np.concatenate((sst_stacked_hist,sst_stacked_ctl,sst_stacked_rcp26),axis=0)\n",
    "X_hist_coda_19th_ctl_rcp26=np.concatenate((sst_stacked_hist,sst_stacked_coda_19th,sst_stacked_ctl,sst_stacked_rcp26),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define Y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_hist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_coda_19th.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_hist_coda_19th_ctl_rcp26=np.concatenate((target_hist,target_coda_19th,target_ctl,target_rcp26),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_hist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_stacked_hist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_stacked_coda_20th.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hist_coda_19th_ctl_rcp26_shuffled,y_hist_coda_19th_ctl_rcp26_shuffled = shuffle(X_hist_coda_19th_ctl_rcp26,y_hist_coda_19th_ctl_rcp26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hist_coda_19th_ctl_rcp26_shuffled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hist_coda_19th_ctl_rcp26_shuffled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nchannels=X_hist_coda_19th_ctl_rcp26_shuffled.shape[3]\n",
    "N_train=X_hist_coda_19th_ctl_rcp26_shuffled.shape[0]\n",
    "N_test=sst_stacked_coda_20th.shape[0]\n",
    "nlat=X_hist_coda_19th_ctl_rcp26_shuffled.shape[1]\n",
    "nlon=X_hist_coda_19th_ctl_rcp26_shuffled.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reshape data for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hist_coda_19th_ctl_rcp26_shuffled4reg=X_hist_coda_19th_ctl_rcp26_shuffled.reshape((N_train,nlat*nlon))\n",
    "y_hist_coda_19th_ctl_rcp26_shuffled4reg=y_hist_coda_19th_ctl_rcp26_shuffled.reshape((N_train,nlat*nlon))\n",
    "sst_stacked_coda_20th4reg=sst_stacked_coda_20th.reshape((N_test,nlat*nlon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hist_coda_19th_ctl_rcp26_shuffled4reg_train=X_hist_coda_19th_ctl_rcp26_shuffled4reg[:round(X_hist_coda_19th_ctl_rcp26_shuffled4reg.shape[0]*0.8),:]\n",
    "y_hist_coda_19th_ctl_rcp26_shuffled4reg_train=y_hist_coda_19th_ctl_rcp26_shuffled4reg[:round(y_hist_coda_19th_ctl_rcp26_shuffled4reg.shape[0]*0.8),:]\n",
    "X_hist_coda_19th_ctl_rcp26_shuffled4reg_valid=X_hist_coda_19th_ctl_rcp26_shuffled4reg[round(X_hist_coda_19th_ctl_rcp26_shuffled4reg.shape[0]*0.8):,:]\n",
    "y_hist_coda_19th_ctl_rcp26_shuffled4reg_valid=y_hist_coda_19th_ctl_rcp26_shuffled4reg[round(y_hist_coda_19th_ctl_rcp26_shuffled4reg.shape[0]*0.8):,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hist_coda_19th_ctl_rcp26_shuffled4reg_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr=linear_model.LinearRegression(n_jobs=1)\n",
    "trained_regr=regr.fit(X_hist_coda_19th_ctl_rcp26_shuffled4reg_train,y_hist_coda_19th_ctl_rcp26_shuffled4reg_train)\n",
    "\n",
    "prediction_valid=trained_regr.predict(X_hist_coda_19th_ctl_rcp26_shuffled4reg_valid)\n",
    "prediction_valid_2d=prediction_valid.reshape(X_hist_coda_19th_ctl_rcp26_shuffled4reg_valid.shape[0],nlat,nlon)\n",
    "\n",
    "prediction=trained_regr.predict(sst_stacked_coda_20th4reg)\n",
    "prediction_2d=prediction.reshape(N_test,nlat,nlon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_4corr=y_hist_coda_19th_ctl_rcp26_shuffled4reg_valid.reshape(X_hist_coda_19th_ctl_rcp26_shuffled4reg_valid.shape[0],nlat*nlon)\n",
    "structure_dummy= np.arange(y_valid_4corr.shape[1], dtype=float)\n",
    "structure_dummy.shape\n",
    "for a in range(1,y_valid_4corr.shape[1]):\n",
    "    one_R=np.corrcoef(prediction_valid[:,a], y_valid_4corr[:,a])\n",
    "    structure_dummy[a]=one_R[0,1]\n",
    "corr_matrix=structure_dummy.reshape(nlat,nlon)\n",
    "corr_matrix.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_4corr=target_coda_20th.reshape(N_test,nlat*nlon)\n",
    "structure_dummy= np.arange(y_test_4corr.shape[1], dtype=float)\n",
    "structure_dummy.shape\n",
    "for a in range(1,y_test_4corr.shape[1]):\n",
    "    one_R=np.corrcoef(prediction[:,a], y_test_4corr[:,a])\n",
    "    structure_dummy[a]=one_R[0,1]\n",
    "corr_matrix=structure_dummy.reshape(nlat,nlon)\n",
    "corr_matrix.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define deep learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeriodicPadding2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, pad_width, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pad_width = pad_width\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if self.pad_width == 0:\n",
    "            return inputs\n",
    "        inputs_padded = tf.concat(\n",
    "            [inputs[:, :, -self.pad_width:, :], inputs, inputs[:, :, :self.pad_width, :]], axis=2)\n",
    "        # Zero padding in the lat direction\n",
    "        inputs_padded = tf.pad(inputs_padded, [[0, 0], [self.pad_width, self.pad_width], [0, 0], [0, 0]])\n",
    "        return inputs_padded\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'pad_width': self.pad_width})\n",
    "        return config\n",
    "\n",
    "\n",
    "class PeriodicConv2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters,\n",
    "                 kernel_size,\n",
    "                 conv_kwargs={},\n",
    "                 **kwargs, ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv_kwargs = conv_kwargs\n",
    "        if type(kernel_size) is not int:\n",
    "            assert kernel_size[0] == kernel_size[1], 'PeriodicConv2D only works for square kernels'\n",
    "            kernel_size = kernel_size[0]\n",
    "        pad_width = (kernel_size - 1) // 2\n",
    "        self.padding = PeriodicPadding2D(pad_width)\n",
    "        self.conv = Conv2D(\n",
    "            filters, kernel_size, padding='valid', **conv_kwargs\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.conv(self.padding(inputs))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'filters': self.filters, 'kernel_size': self.kernel_size, 'conv_kwargs': self.conv_kwargs})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(filters, kernels, input_shape, dr=0):\n",
    "    \"\"\"Fully convolutional network\"\"\"\n",
    "    x = input = Input(shape=input_shape)\n",
    "    for f, k in zip(filters[:-1], kernels[:-1]):\n",
    "        x = PeriodicConv2D(f, k)(x)\n",
    "        x = LeakyReLU()(x)\n",
    "        if dr > 0: x = Dropout(dr)(x)\n",
    "    output = PeriodicConv2D(filters[-1], kernels[-1])(x)\n",
    "    return keras.models.Model(input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_pool(filters, kernels, input_shape, dr=0):\n",
    "    \"\"\"Fully convolutional network\"\"\"\n",
    "    x = input = Input(shape=input_shape)\n",
    "    for f, k in zip(filters[:-1], kernels[:-1]):\n",
    "        x = PeriodicConv2D(f, k)(x)\n",
    "        x = LeakyReLU()(x)\n",
    "        x = MaxPooling2D((2, 2))(x)\n",
    "        if dr > 0: x = Dropout(dr)(x)\n",
    "    output = PeriodicConv2D(filters[-1], kernels[-1])(x)\n",
    "    return keras.models.Model(input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unet(input_shape, kernels,start_filters):\n",
    "    x = input = Input(shape=input_shape)\n",
    "    conv1 = PeriodicConv2D(filters=start_filters* 1, kernel_size=kernels)(x)\n",
    "    conv1 = LeakyReLU()(conv1)\n",
    "    pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "    pool1 = Dropout(0.6)(pool1)\n",
    "\n",
    "    conv2 = PeriodicConv2D(filters=start_filters* 2, kernel_size=kernels)(pool1)\n",
    "    conv2 = LeakyReLU()(conv2)\n",
    "    pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "    pool2 = Dropout(0.6)(pool2)\n",
    "\n",
    "    conv3 = PeriodicConv2D(filters=start_filters* 3, kernel_size=kernels)(pool2)\n",
    "    conv3 = LeakyReLU()(conv3)\n",
    "    pool3 = MaxPooling2D((2, 2))(conv3)\n",
    "    pool3 = Dropout(0.6)(pool3)\n",
    "\n",
    "    conv4 = PeriodicConv2D(filters=start_filters* 4, kernel_size=kernels)(pool3)\n",
    "    conv4 = LeakyReLU()(conv4)\n",
    "    pool4 = MaxPooling2D((2, 2))(conv4)\n",
    "    pool4 = Dropout(0.6)(pool4)\n",
    "\n",
    "    convm = Flatten()(pool4)\n",
    "    convm = Dense(288,activation=\"relu\")(convm)\n",
    "    convm = Dropout(0.7)(convm)\n",
    "    convm = Dense(3*6*64,activation=\"relu\")(convm)\n",
    "    convm = Dropout(0.7)(convm)\n",
    "\n",
    "\n",
    "    t4 = tf.reshape(convm, shape=[-1, 3, 6, 64])\n",
    "    deconv4 = tf.image.resize(t4,conv4.shape[1:3],method=resize_method)   \n",
    "    uconv4 = concatenate([deconv4, pool3])\n",
    "    uconv4 = PeriodicConv2D(filters=start_filters* 3, kernel_size=kernels)(uconv4)\n",
    "    uconv4 = LeakyReLU()(uconv4)\n",
    "\n",
    "    deconv3 = tf.image.resize(uconv4,conv3.shape[1:3],method=resize_method) \n",
    "    uconv3 = concatenate([deconv3, pool2])\n",
    "    uconv3 = PeriodicConv2D(filters=start_filters * 2, kernel_size=kernels)(uconv3)\n",
    "    uconv3 = LeakyReLU()(uconv3)\n",
    "    \n",
    "    deconv2 = tf.image.resize(uconv3,conv2.shape[1:3],method=resize_method)   \n",
    "    uconv2 = concatenate([deconv2, pool1])\n",
    "    uconv2 = PeriodicConv2D(filters=start_filters * 1, kernel_size=kernels)(uconv2)\n",
    "    uconv2 = LeakyReLU()(uconv2)\n",
    "    \n",
    "    deconv1 = tf.image.resize(uconv2,conv1.shape[1:3],method=resize_method)   \n",
    "    uconv1 = concatenate([deconv1, x])\n",
    "    output_layer= PeriodicConv2D(filters=1, kernel_size=kernels)(uconv1)\n",
    "    output = LeakyReLU()(output_layer)\n",
    "\n",
    "    return keras.models.Model(input, output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unet(input_shape, kernels,start_filters):\n",
    "    x = input = Input(shape=input_shape)\n",
    "    conv1 = PeriodicConv2D(filters=start_filters* 1, kernel_size=kernels)(x)\n",
    "    conv1 = LeakyReLU()(conv1)\n",
    "    pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "    pool1 = Dropout(0.6)(pool1)\n",
    "\n",
    "    conv2 = PeriodicConv2D(filters=start_filters* 2, kernel_size=kernels)(pool1)\n",
    "    conv2 = LeakyReLU()(conv2)\n",
    "    pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "    pool2 = Dropout(0.6)(pool2)\n",
    "\n",
    "    conv3 = PeriodicConv2D(filters=start_filters* 3, kernel_size=kernels)(pool2)\n",
    "    conv3 = LeakyReLU()(conv3)\n",
    "    pool3 = MaxPooling2D((2, 2))(conv3)\n",
    "    pool3 = Dropout(0.6)(pool3)\n",
    "\n",
    "    conv4 = PeriodicConv2D(filters=start_filters* 4, kernel_size=kernels)(pool3)\n",
    "    conv4 = LeakyReLU()(conv4)\n",
    "    pool4 = MaxPooling2D((2, 2))(conv4)\n",
    "    pool4 = Dropout(0.6)(pool4)\n",
    "\n",
    "    convm = Flatten()(pool4)\n",
    "    convm = Dense(288,activation=\"relu\")(convm)\n",
    "    convm = Dropout(0.7)(convm)\n",
    "    convm = Dense(3*6*64,activation=\"relu\")(convm)\n",
    "    convm = Dropout(0.7)(convm)\n",
    "\n",
    "\n",
    "    t4 = tf.reshape(convm, shape=[-1, 3, 6, 64])\n",
    "    deconv4 = tf.image.resize(t4,conv4.shape[1:3],method=resize_method)   \n",
    "    uconv4 = concatenate([deconv4, pool3])\n",
    "    uconv4 = PeriodicConv2D(filters=start_filters* 3, kernel_size=kernels)(uconv4)\n",
    "    uconv4 = LeakyReLU()(uconv4)\n",
    "\n",
    "    deconv3 = tf.image.resize(uconv4,conv3.shape[1:3],method=resize_method) \n",
    "    uconv3 = concatenate([deconv3, pool2])\n",
    "    uconv3 = PeriodicConv2D(filters=start_filters * 2, kernel_size=kernels)(uconv3)\n",
    "    uconv3 = LeakyReLU()(uconv3)\n",
    "    \n",
    "    deconv2 = tf.image.resize(uconv3,conv2.shape[1:3],method=resize_method)   \n",
    "    uconv2 = concatenate([deconv2, pool1])\n",
    "    uconv2 = PeriodicConv2D(filters=start_filters * 1, kernel_size=kernels)(uconv2)\n",
    "    uconv2 = LeakyReLU()(uconv2)\n",
    "    \n",
    "    deconv1 = tf.image.resize(uconv2,conv1.shape[1:3],method=resize_method)   \n",
    "    uconv1 = concatenate([deconv1, x])\n",
    "    output_layer= PeriodicConv2D(filters=1, kernel_size=kernels)(uconv1)\n",
    "    output = LeakyReLU()(output_layer)\n",
    "\n",
    "    return keras.models.Model(input, output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unet_small(input_shape, kernels,start_filters):\n",
    "    x = input = Input(shape=input_shape)\n",
    "    conv1 = PeriodicConv2D(filters=start_filters* 1, kernel_size=kernels)(x)\n",
    "    conv1 = LeakyReLU()(conv1)\n",
    "    pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "    pool1 = Dropout(0.6)(pool1)\n",
    "\n",
    "    conv2 = PeriodicConv2D(filters=start_filters* 2, kernel_size=kernels)(pool1)\n",
    "    conv2 = LeakyReLU()(conv2)\n",
    "    pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "    pool2 = Dropout(0.6)(pool2)\n",
    "    \n",
    "    convm = Flatten()(pool2)\n",
    "    convm = Dense(256,activation=\"relu\")(convm)\n",
    "    convm = Dropout(0.5)(convm)\n",
    "    convm = Dense(1*2*64,activation=\"relu\")(convm)\n",
    "    convm = Dropout(0.5)(convm)\n",
    "\n",
    "\n",
    "    t4 = tf.reshape(convm, shape=[-1, 1, 2, 64])\n",
    "    \n",
    "    deconv2 = tf.image.resize(t4,conv2.shape[1:3],method=resize_method)   \n",
    "    uconv2 = concatenate([deconv2, pool1])\n",
    "    uconv2 = PeriodicConv2D(filters=start_filters * 1, kernel_size=kernels)(uconv2)\n",
    "    uconv2 = LeakyReLU()(uconv2)\n",
    "    \n",
    "    deconv1 = tf.image.resize(uconv2,conv1.shape[1:3],method=resize_method)   \n",
    "    uconv1 = concatenate([deconv1, x])\n",
    "    output_layer= PeriodicConv2D(filters=1, kernel_size=kernels)(uconv1)\n",
    "    output = LeakyReLU()(output_layer)\n",
    "\n",
    "    return keras.models.Model(input, output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn = build_cnn([64, 64, 64, 64, 1], [5, 5, 5, 5, 5], (48, 96, 2),dr=0.1)\n",
    "cnn1 = build_cnn([64, 64, 64, 64, 1], [5, 5, 5, 5, 5], (nlat, nlon, nchannels),dr=0.1)\n",
    "cnn1x2 = build_cnn([64, 64, 64, 64, 1], [5, 5, 5, 5, 5], (nlat, nlon, nchannels),dr=0.1)\n",
    "\n",
    "cnn11 = build_cnn([64, 64, 64, 64, 1], [5, 5, 5, 5, 5], (nlat, nlon, nchannels),dr=0.1)\n",
    "cnn11_pool = build_cnn_pool([64, 64, 64, 64, 1], [5, 5, 5, 5, 5], (nlat, nlon, nchannels),dr=0.4)\n",
    "\n",
    "cnn11xxx = build_cnn([32, 64, 1], [5, 5, 5], (nlat, nlon, nchannels),dr=0.4)\n",
    "\n",
    "cnn11kiri= build_cnn([128, 8, 1], [5, 5, 5], (nlat, nlon, nchannels),dr=0.3)# for t2m_DJF dr=0.1, for the rest dr=0.3\n",
    "\n",
    "\n",
    "cnn44 = build_cnn([32, 64, 128, 256, 1], [5, 5, 5, 5, 5], (nlat, nlon, nchannels),dr=0.3)# for t2m_DJF dr=0.2, for the rest dr=0.3\n",
    "cnn44_pool = build_cnn_pool([32, 64, 128, 256, 1], [5, 5, 5, 5, 1], (nlat, nlon, 2),dr=0.4)\n",
    "\n",
    "cnn44_acc = build_cnn([32, 64, 128, 256, 1], [5, 5, 5, 5, 5], (nlat, nlon, nchannels),dr=0.4)\n",
    "\n",
    "\n",
    "\n",
    "cnn5 = build_cnn([64, 64, 64, 64, 1], [3, 3, 3, 3, 3], (nlat, nlon, nchannels),dr=0.1)\n",
    "cnn55 = build_cnn([64, 64, 64, 64, 1], [3, 3, 3, 3, 3], (nlat, nlon, nchannels),dr=0.4)\n",
    "#cnn66 = build_cnn([64, 64, 64, 64, 1], [5, 5, 5, 5, 5], (nlat, nlon, nchannels),dr=0.4)\n",
    "\n",
    "cnn7 = build_cnn([64, 64, 1], [5, 5, 5], (nlat, nlon, nchannels),dr=0.3)# for t2m_DJF dr=0.1, for the rest dr=0.3\n",
    "cnn8 = build_cnn([64, 64, 1], [3, 3, 3], (nlat, nlon, nchannels),dr=0.3)# for t2m_DJF dr=0.1, for the rest dr=0.3\n",
    "\n",
    "unet = build_unet(input_shape=(nlat, nlon, nchannels),kernels=3, start_filters=16)\n",
    "unet1 = build_unet(input_shape=(nlat, nlon, nchannels),kernels=5, start_filters=16)\n",
    "unet2 = build_unet(input_shape=(nlat, nlon, nchannels),kernels=3, start_filters=32)\n",
    "\n",
    "\n",
    "unet2_small = build_unet_small(input_shape=(nlat, nlon, nchannels),kernels=3, start_filters=32)\n",
    "unet3_small = build_unet_small(input_shape=(nlat, nlon, nchannels),kernels=5, start_filters=32)\n",
    "unet4_small = build_unet_small(input_shape=(nlat, nlon, nchannels),kernels=5, start_filters=16)\n",
    "\n",
    "unet_small = build_unet_small(input_shape=(nlat, nlon, nchannels),kernels=3, start_filters=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hist_coda_19th_ctl_rcp26_shuffled4RNN=X_hist_coda_19th_ctl_rcp26_shuffled.reshape((N_train,nlat,nlon))\n",
    "y_hist_coda_19th_ctl_rcp26_shuffled4RNN=y_hist_coda_19th_ctl_rcp26_shuffled.reshape((N_train,nlat,nlon))\n",
    "sst_stacked_coda_20th4RNN=sst_stacked_coda_20th.reshape((N_test,nlat,nlon))\n",
    "\n",
    "y_test_4RNN=target_coda_20th.reshape((N_test,nlat*nlon))\n",
    "structure_4RNN= np.arange(y_test_4RNN.shape[1], dtype=float)\n",
    "structure_4RNN.shape\n",
    "\n",
    "RNN1=Sequential()\n",
    "RNN1.add(LSTM(128,input_shape=(nlat,nlon),dropout=0.7,activation=\"tanh\",unroll=True)) # for t2m_DJF dr=0.4, for the rest dr=0.7\n",
    "RNN1.add(Dense(nlat*nlon,activation=\"linear\"))\n",
    "RNN1.add(Reshape((nlat,nlon)))\n",
    "RNN1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "cnn1x2.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "\n",
    "\n",
    "RNN1.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "\n",
    "\n",
    "cnn11.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "cnn11xxx.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "\n",
    "cnn11kiri.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "\n",
    "\n",
    "cnn11_pool.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "\n",
    "cnn44.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "cnn44_acc.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"accuracy\"])\n",
    "\n",
    "cnn44_pool.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "\n",
    "cnn5.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "cnn55.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "cnn7.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "cnn8.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "#cnn66.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "unet1.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "unet2.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "unet_small.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "unet2_small.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "unet3_small.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "unet4_small.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ess = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "\n",
    "ess_rnn = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mc_CNN1_check=readin_data+'best_model_mpi_nov_sst_'+target+'_CNN1_m'+stat_model_member\n",
    "mc_RNN1_check=readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_RNN1_m'+stat_model_member\n",
    "mc_auto3_check=readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_auto3_m'+stat_model_member\n",
    "mc_auto4_check=readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_auto4_m'+stat_model_member\n",
    "mc_auto5_check=readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_auto5_m'+stat_model_member\n",
    "mc_auto1_check=readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_auto1_m'+stat_model_member\n",
    "mc_auto2_check=readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_auto2_m'+stat_model_member\n",
    "mc_CNN11_check=readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_CNN11_m'+stat_model_member\n",
    "mc_CNN11kiri_check=readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_CNN11_kiri_m'+stat_model_member\n",
    "mc_modelC_check=readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_modelC_m'+stat_model_member\n",
    "mc_CNN44_check=readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_CNN44_m'+stat_model_member\n",
    "mc_CNN7_check=readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_CNN7_m'+stat_model_member\n",
    "mc_CNN8_check=readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_CNN8_m'+stat_model_member\n",
    "mc_unet_check=readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_unet_m'+stat_model_member\n",
    "mc_unet1_check=readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_unet1_m'+stat_model_member\n",
    "mc_unet2_check=readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_unet2_m'+stat_model_member\n",
    "\n",
    "mc_RNN1 = ModelCheckpoint(readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_RNN2_m'+stat_model_member+'.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "\n",
    "\n",
    "mc_CNN11 = ModelCheckpoint(readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_CNN11_m'+stat_model_member+'.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "mc_CNN11kiri = ModelCheckpoint(readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_CNN11kiri_m'+stat_model_member+'.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "\n",
    "mc_CNN44 = ModelCheckpoint(readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_CNN44_m'+stat_model_member+'.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "mc_CNN7 = ModelCheckpoint(readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_CNN7_m'+stat_model_member+'.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "mc_CNN8 = ModelCheckpoint(readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_CNN8_m'+stat_model_member+'.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "cmc_unet1 = ModelCheckpoint(readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_unet1_m'+stat_model_member+'.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "mc_unet2 = ModelCheckpoint(readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_unet2_m'+stat_model_member+'.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "mc_unet = ModelCheckpoint(readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_unet_m'+stat_model_member+'.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "\n",
    "mc_unet_small = ModelCheckpoint(readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_unet1_small_m'+stat_model_member+'.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "mc_unet2_small = ModelCheckpoint(readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_unet2_small_m'+stat_model_member+'.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "mc_unet3_small = ModelCheckpoint(readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_unet3_small_m'+stat_model_member+'.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "mc_unet4_small = ModelCheckpoint(readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_unet4_small_m'+stat_model_member+'.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history_RNN1=RNN1.fit(X_hist_coda_19th_ctl_rcp26_shuffled4RNN,y_hist_coda_19th_ctl_rcp26_shuffled4RNN,epochs=3000,batch_size=128,validation_split=0.2, callbacks=[ess_rnn, mc_RNN1])\n",
    "\n",
    "val_loss_plot(history=history_RNN1,folder=save_folder,name=target+'_RNN2_m'+stat_model_member)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history_unet_small=unet_small.fit(X_hist_coda_19th_ctl_rcp26_shuffled,y_hist_coda_19th_ctl_rcp26_shuffled,epochs=3000,batch_size=128,validation_split=0.2, callbacks=[ess, mc_unet_small])\n",
    "\n",
    "val_loss_plot(history=history_unet_small,folder=save_folder,name=target+'_unet_small_m'+stat_model_member)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history_unet2_small=unet2_small.fit(X_hist_coda_19th_ctl_rcp26_shuffled,y_hist_coda_19th_ctl_rcp26_shuffled,epochs=3000,batch_size=128,validation_split=0.2, callbacks=[ess, mc_unet2_small])\n",
    "\n",
    "val_loss_plot(history=history_unet2_small,folder=save_folder,name=target+'_unet2_small_m'+stat_model_member)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history_unet3_small=unet3_small.fit(X_hist_coda_19th_ctl_rcp26_shuffled,y_hist_coda_19th_ctl_rcp26_shuffled,epochs=3000,batch_size=128,validation_split=0.2, callbacks=[ess, mc_unet3_small])\n",
    "\n",
    "val_loss_plot(history=history_unet3_small,folder=save_folder,name=target+'_unet3_small_m'+stat_model_member)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history_unet4_small=unet4_small.fit(X_hist_coda_19th_ctl_rcp26_shuffled,y_hist_coda_19th_ctl_rcp26_shuffled,epochs=3000,batch_size=128,validation_split=0.2, callbacks=[ess, mc_unet4_small])\n",
    "\n",
    "val_loss_plot(history=history_unet4_small,folder=save_folder,name=target+'_unet4_small_m'+stat_model_member)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_cnn44=cnn44.fit(X_hist_coda_19th_ctl_rcp26_shuffled,y_hist_coda_19th_ctl_rcp26_shuffled,epochs=3000,batch_size=128,validation_split=0.2, callbacks=[ess, mc_cnn44])\n",
    "val_loss_plot(history=history_cnn44,folder=save_folder,name=target+'_cnn44_m'+stat_model_member)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_cnn11kiri=cnn11kiri.fit(X_hist_coda_19th_ctl_rcp26_shuffled,y_hist_coda_19th_ctl_rcp26_shuffled,epochs=3000,batch_size=128,validation_split=0.2, callbacks=[ess, mc_cnn11kiri])\n",
    "val_loss_plot(history=history_cnn11kiri,folder=save_folder,name=target+'_cnn11kiri_m'+stat_model_member)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_cnn8=cnn8.fit(X_hist_coda_19th_ctl_rcp26_shuffled,y_hist_coda_19th_ctl_rcp26_shuffled,epochs=3000,batch_size=128,validation_split=0.2, callbacks=[ess, mc_cnn8])\n",
    "val_loss_plot(history=history_cnn8,folder=save_folder,name=target+'_cnn8_m'+stat_model_member)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_cnn7=cnn7.fit(X_hist_coda_19th_ctl_rcp26_shuffled,y_hist_coda_19th_ctl_rcp26_shuffled,epochs=3000,batch_size=128,validation_split=0.2, callbacks=[ess, mc_cnn7])\n",
    "val_loss_plot(history=history_cnn7,folder=save_folder,name=target+'_cnn7_m'+stat_model_member)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history_unet2=unet2.fit(X_hist_coda_19th_ctl_rcp26_shuffled,y_hist_coda_19th_ctl_rcp26_shuffled,epochs=3000,batch_size=128,validation_split=0.2, callbacks=[ess, mc_unet2])\n",
    "\n",
    "val_loss_plot(history=history_unet2,folder=save_folder,name=target+'_unet2_m'+stat_model_member)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history_unet1=unet1.fit(X_hist_coda_19th_ctl_rcp26_shuffled,y_hist_coda_19th_ctl_rcp26_shuffled,epochs=3000,batch_size=128,validation_split=0.2, callbacks=[ess, mc_unet1])\n",
    "\n",
    "val_loss_plot(history=history_unet1,folder=save_folder,name=target+'_unet1_m'+stat_model_member)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history_unet=unet.fit(X_hist_coda_19th_ctl_rcp26_shuffled,y_hist_coda_19th_ctl_rcp26_shuffled,epochs=3000,batch_size=128,validation_split=0.2, callbacks=[ess, mc_unet])\n",
    "\n",
    "val_loss_plot(history=history_unet,folder=save_folder,name=target+'_unet_m'+stat_model_member)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check over under fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_list=[]\n",
    "loss_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss =history_cnn44.history[\"loss\"]\n",
    "val_loss =history_cnn44.history[\"val_loss\"]\n",
    "epochs=range(1,len(loss)+1)\n",
    "\n",
    "print(np.min(val_loss))\n",
    "val_loss_list.append(np.min(val_loss))\n",
    "\n",
    "print(np.min(loss))\n",
    "loss_list.append(np.min(loss))\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs,loss,\"b\",color=\"blue\",label=\"Training loss\")\n",
    "plt.plot(epochs,val_loss,\"b\",color=\"red\",label=\"Validation loss\")\n",
    "plt.title(\"Loss Curves for cnn44\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss =history_RNN1.history[\"loss\"]\n",
    "val_loss =history_RNN1.history[\"val_loss\"]\n",
    "epochs=range(1,len(loss)+1)\n",
    "\n",
    "print(np.min(val_loss))\n",
    "val_loss_list.append(np.min(val_loss))\n",
    "\n",
    "print(np.min(loss))\n",
    "loss_list.append(np.min(loss))\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs,loss,\"b\",color=\"blue\",label=\"Training loss\")\n",
    "plt.plot(epochs,val_loss,\"b\",color=\"red\",label=\"Validation loss\")\n",
    "plt.title(\"Loss Curves for RNN1\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss =history_cnn8.history[\"loss\"]\n",
    "val_loss =history_cnn8.history[\"val_loss\"]\n",
    "epochs=range(1,len(loss)+1)\n",
    "\n",
    "print(np.min(val_loss))\n",
    "val_loss_list.append(np.min(val_loss))\n",
    "\n",
    "print(np.min(loss))\n",
    "loss_list.append(np.min(loss))\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs,loss,\"b\",color=\"blue\",label=\"Training loss\")\n",
    "plt.plot(epochs,val_loss,\"b\",color=\"red\",label=\"Validation loss\")\n",
    "plt.title(\"Loss Curves for cnn8\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss =history_cnn7.history[\"loss\"]\n",
    "val_loss =history_cnn7.history[\"val_loss\"]\n",
    "epochs=range(1,len(loss)+1)\n",
    "\n",
    "print(np.min(val_loss))\n",
    "val_loss_list.append(np.min(val_loss))\n",
    "\n",
    "print(np.min(loss))\n",
    "loss_list.append(np.min(loss))\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs,loss,\"b\",color=\"blue\",label=\"Training loss\")\n",
    "plt.plot(epochs,val_loss,\"b\",color=\"red\",label=\"Validation loss\")\n",
    "plt.title(\"Loss Curves for cnn7\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss =history_cnn11kiri.history[\"loss\"]\n",
    "val_loss =history_cnn11kiri.history[\"val_loss\"]\n",
    "epochs=range(1,len(loss)+1)\n",
    "\n",
    "print(np.min(val_loss))\n",
    "val_loss_list.append(np.min(val_loss))\n",
    "\n",
    "print(np.min(loss))\n",
    "loss_list.append(np.min(loss))\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs,loss,\"b\",color=\"blue\",label=\"Training loss\")\n",
    "plt.plot(epochs,val_loss,\"b\",color=\"red\",label=\"Validation loss\")\n",
    "plt.title(\"Loss Curves for cnn11kiri\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss =history_unet1.history[\"loss\"]\n",
    "val_loss =history_unet1.history[\"val_loss\"]\n",
    "epochs=range(1,len(loss)+1)\n",
    "\n",
    "print(np.min(val_loss))\n",
    "val_loss_list.append(np.min(val_loss))\n",
    "\n",
    "print(np.min(loss))\n",
    "loss_list.append(np.min(loss))\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs,loss,\"b\",color=\"blue\",label=\"Training loss\")\n",
    "plt.plot(epochs,val_loss,\"b\",color=\"red\",label=\"Validation loss\")\n",
    "plt.title(\"Loss Curves for unet1\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss =history_unet2.history[\"loss\"]\n",
    "val_loss =history_unet2.history[\"val_loss\"]\n",
    "epochs=range(1,len(loss)+1)\n",
    "\n",
    "print(np.min(val_loss))\n",
    "val_loss_list.append(np.min(val_loss))\n",
    "\n",
    "print(np.min(loss))\n",
    "loss_list.append(np.min(loss))\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs,loss,\"b\",color=\"blue\",label=\"Training loss\")\n",
    "plt.plot(epochs,val_loss,\"b\",color=\"red\",label=\"Validation loss\")\n",
    "plt.title(\"Loss Curves for unet2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4), dpi= 200)\n",
    "loss =history_unet.history[\"loss\"]\n",
    "val_loss =history_unet.history[\"val_loss\"]\n",
    "epochs=range(1,len(loss)+1)\n",
    "\n",
    "print(np.min(val_loss))\n",
    "val_loss_list.append(np.min(val_loss))\n",
    "\n",
    "print(np.min(loss))\n",
    "loss_list.append(np.min(loss))\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs,loss,\"b\",color=\"blue\",label=\"Training loss\")\n",
    "plt.plot(epochs,val_loss,\"b\",color=\"red\",label=\"Validation loss\")\n",
    "plt.title(\"Loss Curves for unet\")\n",
    "\n",
    "plt.title(\"VL: \"+str(round(np.min(val_loss),2)),loc=\"left\")\n",
    "plt.title(\"TL: \"+str(round(np.min(loss),2)),loc=\"right\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss =history_unet3_small.history[\"loss\"]\n",
    "val_loss =history_unet3_small.history[\"val_loss\"]\n",
    "epochs=range(1,len(loss)+1)\n",
    "\n",
    "print(np.min(val_loss))\n",
    "val_loss_list.append(np.min(val_loss))\n",
    "\n",
    "print(np.min(loss))\n",
    "loss_list.append(np.min(loss))\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs,loss,\"b\",color=\"blue\",label=\"Training loss\")\n",
    "plt.plot(epochs,val_loss,\"b\",color=\"red\",label=\"Validation loss\")\n",
    "plt.title(\"Loss Curves for unet3_small\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss =history_unet2_small.history[\"loss\"]\n",
    "val_loss =history_unet2_small.history[\"val_loss\"]\n",
    "epochs=range(1,len(loss)+1)\n",
    "\n",
    "print(np.min(val_loss))\n",
    "val_loss_list.append(np.min(val_loss))\n",
    "\n",
    "print(np.min(loss))\n",
    "loss_list.append(np.min(loss))\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs,loss,\"b\",color=\"blue\",label=\"Training loss\")\n",
    "plt.plot(epochs,val_loss,\"b\",color=\"red\",label=\"Validation loss\")\n",
    "plt.title(\"Loss Curves for unet2_small\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss =history_unet4_small.history[\"loss\"]\n",
    "val_loss =history_unet4_small.history[\"val_loss\"]\n",
    "epochs=range(1,len(loss)+1)\n",
    "\n",
    "print(np.min(val_loss))\n",
    "val_loss_list.append(np.min(val_loss))\n",
    "\n",
    "print(np.min(loss))\n",
    "loss_list.append(np.min(loss))\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs,loss,\"b\",color=\"blue\",label=\"Training loss\")\n",
    "plt.plot(epochs,val_loss,\"b\",color=\"red\",label=\"Validation loss\")\n",
    "plt.title(\"Loss Curves for unet4_small\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss =history_unet_small.history[\"loss\"]\n",
    "val_loss =history_unet_small.history[\"val_loss\"]\n",
    "epochs=range(1,len(loss)+1)\n",
    "\n",
    "print(np.min(val_loss))\n",
    "val_loss_list.append(np.min(val_loss))\n",
    "\n",
    "print(np.min(loss))\n",
    "loss_list.append(np.min(loss))\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs,loss,\"b\",color=\"blue\",label=\"Training loss\")\n",
    "plt.plot(epochs,val_loss,\"b\",color=\"red\",label=\"Validation loss\")\n",
    "plt.title(\"Loss Curves for unet_small\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(11, 7))\n",
    "plt.plot(val_loss_list,\"bo\",color=\"red\",label=\"Min. Validation loss\")\n",
    "plt.title(\"Minimum Loss\")\n",
    "plt.plot(loss_list,\"bo\",color=\"blue\",label=\"Min. Training loss\")\n",
    "plt.legend()\n",
    "plt.xticks([0, 1, 2,3,4,5,6,7,8,9,10,11], [\"cnn44\",\"RNN1\",\"cnn8\",\"cnn7\",\"cnn11kiri\",\"unet1\",\"unet2\",\"unet\",\"unet3_small\",\"unet2_small\",\"unet4_small\",\"unet_small\"],rotation=90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()\n",
    "fig.savefig(save_folder+\"min_val_loss_\"+stat_model_member+\"_\"+target+\".pdf\")\n",
    "fig.savefig(save_folder+\"min_val_loss_\"+stat_model_member+\"_\"+target+\".png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imac311",
   "language": "python",
   "name": "imac311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

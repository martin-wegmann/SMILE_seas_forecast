{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating winter forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Martin Wegmann\n",
    "\n",
    "Email: martin.wegmann@unibe.ch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to evaluate the predictions made by the different architectures in the other notebook against the 20CRv3 fields in the 20th century."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this notebook is very non-automaized. Time was not a big constraint in my work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example how to do it for the winter season. The exact same structure is done for the summer season, meaning May SSTs as input for JJA T2M and SLP predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow.keras.backend as K\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from tensorflow.keras import models\n",
    "from collections import OrderedDict\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, SimpleRNN, LSTM, GRU, Reshape\n",
    "from keras import initializers\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras import Model, Input, regularizers\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, UpSampling2D, Add, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import datetime\n",
    "from netCDF4 import Dataset,num2date,date2num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_method = tf.image.ResizeMethod.NEAREST_NEIGHBOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_check_txt(text=\"check\"):\n",
    "    with open(text+'.txt', 'w') as f:\n",
    "        f.write(text)\n",
    "    return print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loss_plot(history,folder,name=\"example\"):\n",
    "    fig = plt.figure(figsize=(8, 4), dpi= 200)\n",
    "    loss =history.history[\"loss\"]\n",
    "    val_loss =history.history[\"val_loss\"]\n",
    "    epochs=range(1,len(loss)+1)\n",
    "\n",
    "    print(np.min(val_loss))\n",
    "\n",
    "\n",
    "    print(np.min(loss))\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(epochs,loss,\"b\",color=\"blue\",label=\"Training loss\")\n",
    "    plt.plot(epochs,val_loss,\"b\",color=\"red\",label=\"Validation loss\")\n",
    "    plt.title(\"Loss Curves for \"+name)\n",
    "    plt.title(\"VL: \"+str(round(np.min(val_loss),2)),loc=\"left\")\n",
    "    plt.title(\"TL: \"+str(round(np.min(loss),2)),loc=\"right\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.savefig(folder+\"Loss Curves for \"+name+\".png\")\n",
    "    plt.show()\n",
    "    return print(\"saved in \"+folder+\"Loss Curves for \"+name+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corr(rec,original,name,folder):\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    rec_4corr=rec.reshape(rec.shape[0],rec.shape[1]*rec.shape[2])\n",
    "    original_4corr=original.reshape(rec.shape[0],rec.shape[1]*rec.shape[2])\n",
    "    structure_dummy= np.arange(original_4corr.shape[1], dtype=float)\n",
    "    for a in range(1,original_4corr.shape[1]):\n",
    "        one_R=np.corrcoef(rec_4corr[:,a], original_4corr[:,a])\n",
    "        structure_dummy[a]=one_R[0,1]\n",
    "    corr_matrix=structure_dummy.reshape(rec.shape[1],rec.shape[2])\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "\n",
    "    plt.imshow(corr_matrix, vmin=-1, vmax=1, cmap='seismic',origin='lower',interpolation=\"none\") \n",
    "    plt.title(str(np.nanmean(corr_matrix)),loc=\"left\")\n",
    "    bar = plt.colorbar()\n",
    "\n",
    "    \n",
    "    fig.savefig(name+\"_corr.png\")\n",
    "    plt.show()\n",
    "    print(corr_matrix.mean())\n",
    "    return corr_matrix.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regression_member(X_data,y_data,test_X_data,test_y_data):\n",
    "    X_shuffled,y_shuffled = shuffle(X_data,y_data)\n",
    "    nchannels=X_shuffled.shape[3]\n",
    "    N_train=X_shuffled.shape[0]\n",
    "    nlat=X_shuffled.shape[1]\n",
    "    nlon=X_shuffled.shape[2]\n",
    "    N_test=test_y_data.shape[0]\n",
    "    X_shuffled4reg=X_shuffled.reshape((N_train,nlat*nlon))\n",
    "    y_shuffled4reg=y_shuffled.reshape((N_train,nlat*nlon))\n",
    "    X_shuffled4reg_train=X_shuffled4reg[:round(X_shuffled4reg.shape[0]*0.8),:]\n",
    "    y_shuffled4reg_train=y_shuffled4reg[:round(y_shuffled4reg.shape[0]*0.8),:]\n",
    "    X_shuffled4reg_valid=X_shuffled4reg[round(X_shuffled4reg.shape[0]*0.8):,:]\n",
    "    y_shuffled4reg_valid=y_shuffled4reg[round(y_shuffled4reg.shape[0]*0.8):,:]\n",
    "    \n",
    "    \n",
    "\n",
    "    regr=linear_model.LinearRegression(n_jobs=1)\n",
    "    trained_regr=regr.fit(X_shuffled4reg_train,y_shuffled4reg_train)\n",
    "\n",
    "    prediction_valid=trained_regr.predict(X_shuffled4reg_valid)\n",
    "    prediction_valid_2d=prediction_valid.reshape(X_shuffled4reg_valid.shape[0],nlat,nlon)\n",
    "\n",
    "    prediction_reg=trained_regr.predict(test_X_data)\n",
    "    prediction_2d_reg=prediction_reg.reshape(N_test,nlat,nlon)\n",
    "    trained_regr_score=trained_regr.score(X_shuffled4reg_train,y_shuffled4reg_train)\n",
    "    print(\"trained regres. score: \"+str(trained_regr_score))\n",
    "    \n",
    "    y_valid_4corr=y_shuffled4reg_valid.reshape(X_shuffled4reg_valid.shape[0],nlat*nlon)\n",
    "    structure_dummy= np.arange(y_valid_4corr.shape[1], dtype=float)\n",
    "    \n",
    "    for a in range(1,y_valid_4corr.shape[1]):\n",
    "        one_R=np.corrcoef(prediction_valid[:,a], y_valid_4corr[:,a])\n",
    "        structure_dummy[a]=one_R[0,1]\n",
    "    corr_matrix=structure_dummy.reshape(nlat,nlon)\n",
    "    valid_regr_score=corr_matrix.mean()\n",
    "    print(\"valid regres. score: \"+str( valid_regr_score))\n",
    "    \n",
    "    y_test_4corr=test_y_data.reshape(N_test,nlat*nlon)\n",
    "    structure_dummy= np.arange(y_test_4corr.shape[1], dtype=float)\n",
    "\n",
    "    \n",
    "    for a in range(1,y_test_4corr.shape[1]):\n",
    "        one_R=np.corrcoef(prediction_reg[:,a], y_test_4corr[:,a])\n",
    "        structure_dummy[a]=one_R[0,1]\n",
    "    corr_matrix=structure_dummy.reshape(nlat,nlon)\n",
    "    test_regr_score=corr_matrix.mean()\n",
    "    print(\"test regres. score: \"+str(test_regr_score))\n",
    "    return prediction_2d_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmse(rec,original,name,folder):\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    rec_4corr=rec.reshape(rec.shape[0],rec.shape[1]*rec.shape[2])\n",
    "    original_4corr=original.reshape(rec.shape[0],rec.shape[1]*rec.shape[2])\n",
    "    structure_dummy= np.arange(original_4corr.shape[1], dtype=float)\n",
    "    for a in range(1,original_4corr.shape[1]):\n",
    "        one_RMSE=np.sqrt(np.mean((original_4corr[:,a]-rec_4corr[:,a])**2))\n",
    "        structure_dummy[a]=one_RMSE\n",
    "    RMSE_matrix=structure_dummy.reshape(rec.shape[1],rec.shape[2])\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "\n",
    "    plt.imshow(RMSE_matrix, cmap='viridis',origin='lower',interpolation=\"none\") \n",
    "    plt.title(str(np.nanmean(RMSE_matrix)),loc=\"left\")\n",
    "    bar = plt.colorbar()\n",
    "\n",
    "    \n",
    "    fig.savefig(name+\"_RMSE.png\")\n",
    "    plt.show()\n",
    "    return print(RMSE_matrix.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prediction_netcdf(prediction,name,folder,lons,lats,var = \"t2m\",units = \"K\"):\n",
    "    \n",
    "        #!/usr/bin/env ipython\n",
    "    # ---------------------\n",
    "\n",
    "    # -----------------------\n",
    "    nyears = prediction.shape[0];\n",
    "    unout = 'days since 1800-01-01 00:00:00'\n",
    "    # -----------------------\n",
    "    ny, nx = (prediction.shape[1], prediction.shape[2])\n",
    "    lon = lons\n",
    "    lat = lats\n",
    "    var = var\n",
    "    units = units\n",
    "    output=name+\".nc\"\n",
    "    if os.path.isfile(output):\n",
    "        os.remove(output)\n",
    "\n",
    "    dataout = prediction.reshape(nyears,prediction.shape[1],prediction.shape[2]); # create some random data\n",
    "    datesout = [datetime.datetime(1901+iyear,1,1) for iyear in range(nyears)]; # create datevalues\n",
    "    # =========================\n",
    "    ncout = Dataset(output,'w','NETCDF4'); # using netCDF3 for output format \n",
    "    ncout.createDimension('lon',nx);\n",
    "    ncout.createDimension('lat',ny);\n",
    "    ncout.createDimension('time',nyears);\n",
    "    lonvar = ncout.createVariable('lon','float32',('lon'));lonvar[:] = lon;\n",
    "    latvar = ncout.createVariable('lat','float32',('lat'));latvar[:] = lat;\n",
    "    timevar = ncout.createVariable('time','float64',('time'));timevar.setncattr('units',unout);timevar[:]=date2num(datesout,unout);\n",
    "    myvar = ncout.createVariable(var,'float32',('time','lat','lon'));myvar.setncattr('units',units);myvar[:] = dataout;\n",
    "    \n",
    "    ncout.close();\n",
    "    print(\"File saved at: \"+output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_output(model,output_name,test_data,target,eric=False):\n",
    "    model.load_weights(output_name+\".h5\")\n",
    "    prediction=model.predict(test_data)\n",
    "    if output_name==mc_modelC_check:\n",
    "        if eric==True:\n",
    "            prediction=prediction.reshape(test_data.shape[0],nlat,nlon)\n",
    "        else:\n",
    "            prediction=prediction.reshape(115,nlat,nlon)\n",
    "    if eric==True:\n",
    "        output_name=output_name+\"_eric\"\n",
    "    corr_mean=get_corr(rec=prediction,original=target,folder=save_folder,name=output_name)\n",
    "    get_rmse(rec=prediction,original=target,folder=save_folder,name=output_name)\n",
    "    netcdf_name=save_prediction_netcdf(prediction=prediction,name=output_name,folder=save_folder,lons=lons,lats=lats,var = var_name,units = var_unit)\n",
    "    nc_file=xr.open_dataset(netcdf_name)\n",
    "    return corr_mean,nc_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=\"slp_DJF\"\n",
    "var_name=\"slp\"\n",
    "var_unit=\"Pa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list=[\"t2m_dec\",\"t2m_jan\",\"t2m_feb\",\"t2m_DJF\",\"slp_DJF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN44_dic = {\"t2m_dec\": 0.3, \"t2m_jan\": 0.3,\"t2m_feb\": 0.3,\"t2m_DJF\": 0.2,\"slp_DJF\": 0.3}\n",
    "CNN8_dic = {\"t2m_dec\": 0.3, \"t2m_jan\": 0.3,\"t2m_feb\": 0.3,\"t2m_DJF\": 0.1,\"slp_DJF\": 0.3}\n",
    "CNN7_dic = {\"t2m_dec\": 0.3, \"t2m_jan\": 0.3,\"t2m_feb\": 0.3,\"t2m_DJF\": 0.1,\"slp_DJF\": 0.3}\n",
    "CNN11kiri_dic = {\"t2m_dec\": 0.3, \"t2m_jan\": 0.3,\"t2m_feb\": 0.3,\"t2m_DJF\": 0.1,\"slp_DJF\": 0.3}\n",
    "RNN1_dic = {\"t2m_dec\": 0.7, \"t2m_jan\": 0.7,\"t2m_feb\": 0.7,\"t2m_DJF\": 0.4,\"slp_DJF\": 0.7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##for summer\n",
    "#CNN44_dic = {\"t2m_jun\": 0.2, \"t2m_jul\": 0.2,\"t2m_aug\": 0.2,\"t2m_JJA\": 0.2,\"slp_JJA\": 0.3}\n",
    "#CNN8_dic = {\"t2m_jun\": 0.1, \"t2m_jul\": 0.1,\"t2m_aug\": 0.1,\"t2m_JJA\": 0.1,\"slp_JJA\": 0.3}\n",
    "#CNN7_dic = {\"t2m_jun\": 0.1, \"t2m_jul\": 0.1,\"t2m_aug\": 0.1,\"t2m_JJA\": 0.1,\"slp_JJA\": 0.3}\n",
    "#CNN11kiri_dic = {\"t2m_jun\": 0.1, \"t2m_jul\": 0.1,\"t2m_aug\": 0.1,\"t2m_JJA\": 0.1,\"slp_JJA\": 0.3}\n",
    "#RNN1_dic = {\"t2m_jun\": 0.4, \"t2m_jul\": 0.4,\"t2m_aug\": 0.4,\"t2m_JJA\": 0.4,\"slp_JJA\": 0.4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_model_member=\"01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_model_member_list=[\"01\",\"02\",\"03\",\"04\",\"05\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN44_dic[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder=\"/storage/homefs/mawegmann/data/\"\n",
    "readin_mpige=\"/storage/homefs/mawegmann/data/mpi_ge/\"\n",
    "readin_data=\"/storage/homefs/mawegmann/data/mpi_ge/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read in normalized, masked SSTs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kind of data sets do we have in here?\n",
    "\n",
    "* MPI-GE Control\n",
    "\n",
    "* MPI-GE Hist\n",
    "\n",
    "* MPI-GE RCP26\n",
    "\n",
    "* CODA SSTs 1850-1899 for training\n",
    "\n",
    "* CODA SSTs 1900-2015 for testing\n",
    "\n",
    "We focus on November mean SSTs here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mpi ge ssts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetao_nov = xr.open_dataset(readin_mpige+\"thetao7_OImon_traintest_nov_lowlow_supermasked_anomnorm.nc\")\n",
    "thetao_nov = thetao_nov.thetao.drop('lev')\n",
    "thetao_nov= thetao_nov.squeeze('lev')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetao_nov_ctl = xr.open_dataset(readin_mpige+\"thetao_control_nov_lowlow_supermasked_anomnorm.nc\")\n",
    "thetao_nov_ctl = thetao_nov_ctl.thetao.drop('lev')\n",
    "thetao_nov_ctl= thetao_nov_ctl.squeeze('lev')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetao_nov_rcp26 = xr.open_dataset(readin_mpige+\"thetao7_OImon_traintest_nov_lowlow_supermasked_rcp26_anomnorm.nc\")\n",
    "thetao_nov_rcp26 = thetao_nov_rcp26.thetao.drop('lev')\n",
    "thetao_nov_rcp26= thetao_nov_rcp26.squeeze('lev')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coda ssts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetao_nov_codaearly = xr.open_dataset(readin_data+\"sst.mon.mean_18501899_nov_lowlow_supermasked_anomnorm.nc\")\n",
    "thetao_nov_codaearly = thetao_nov_codaearly.sst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetao_nov_codalate = xr.open_dataset(readin_data+\"sst.mon.mean_19002014_nov_lowlow_supermasked_anomnorm.nc\")\n",
    "thetao_nov_codalate  = thetao_nov_codalate.sst\n",
    "\n",
    "thetao_nov_codalate_refwindow=thetao_nov_codalate[51:81,:,:]\n",
    "thetao_nov_codalate_climate=thetao_nov_codalate_refwindow.mean(\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in normalized Atmospheric Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kind of data sets do we have in here?\n",
    "\n",
    "* MPI-GE Control\n",
    "\n",
    "* MPI-GE Hist\n",
    "\n",
    "* MPI-GE RCP26\n",
    "\n",
    "* 20CRv3 ensmean 1850-1899 for training\n",
    "\n",
    "* 20Crv3 ensmean 1900-2015 for testing\n",
    "\n",
    "We focus on December, January and February fields here as well as the DJF mean. \n",
    "\n",
    "We just look at 2m temperature and Sea Level Pressure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All training datasets are normalized with the same mean and standard deviation (coming from Mpi-GE).\n",
    "The test data is normalized by the mean and standard deviation of the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mpi ge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_DJF = xr.open_dataset(readin_mpige+\"tas_Amon_MPI-ESM_historical_all_i1850p3_185001-200512_traintest_DJF_lowlow_anomnorm.nc\")\n",
    "t2m_DJF =t2m_DJF.tas\n",
    "t2m_DJF = t2m_DJF[:15190,:,:]\n",
    "lons=t2m_DJF.lon\n",
    "lats=t2m_DJF.lat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_dec= xr.open_dataset(readin_mpige+\"tas_Amon_MPI-ESM_historical_all_i1850p3_185001-200512_traintest_dec_lowlow_anomnorm.nc\")\n",
    "t2m_dec =t2m_dec.tas\n",
    "t2m_dec = t2m_dec[:15190,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_jan = xr.open_dataset(readin_mpige+\"tas_Amon_MPI-ESM_historical_all_i1850p3_185001-200512_traintest_jan_lowlow_anomnorm.nc\")\n",
    "t2m_jan =t2m_jan.tas\n",
    "t2m_jan = t2m_jan[:15190,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_feb = xr.open_dataset(readin_mpige+\"tas_Amon_MPI-ESM_historical_all_i1850p3_185001-200512_traintest_feb_lowlow_anomnorm.nc\")\n",
    "t2m_feb =t2m_feb.tas\n",
    "t2m_feb = t2m_feb[:15190,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slp_DJF = xr.open_dataset(readin_mpige+\"psl_Amon_MPI-ESM_historical_all_i1850p3_185001-200512_traintest_DJF_lowlow_anomnorm.nc\")\n",
    "slp_DJF =slp_DJF.psl\n",
    "slp_DJF = slp_DJF[:15190,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_DJF_ctl = xr.open_dataset(readin_mpige+\"tas_Amon_mpige_control_DJF_lowlow_anomnorm.nc\")\n",
    "t2m_DJF_ctl =t2m_DJF_ctl.tas\n",
    "\n",
    "#slp_DJF=slp_DJF.reset_index(\"time\",drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_dec_ctl = xr.open_dataset(readin_mpige+\"tas_Amon_mpige_control_dec_lowlow_anomnorm.nc\")\n",
    "t2m_dec_ctl =t2m_dec_ctl.tas\n",
    "\n",
    "#slp_DJF=slp_DJF.reset_index(\"time\",drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_jan_ctl = xr.open_dataset(readin_mpige+\"tas_Amon_mpige_control_jan_lowlow_anomnorm.nc\")\n",
    "t2m_jan_ctl =t2m_jan_ctl.tas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_feb_ctl = xr.open_dataset(readin_mpige+\"tas_Amon_mpige_control_feb_lowlow_anomnorm.nc\")\n",
    "t2m_feb_ctl =t2m_feb_ctl.tas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slp_DJF_ctl = xr.open_dataset(readin_mpige+\"psl_Amon_mpige_control_DJF_lowlow_anomnorm.nc\")\n",
    "slp_DJF_ctl =slp_DJF_ctl.psl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_DJF_rcp26 = xr.open_dataset(readin_mpige+\"tas_Amon_MPI-ESM_rcp26_all_i2005p3_200601-209912_traintest_DJF_lowlow_anomnorm.nc\")\n",
    "t2m_DJF_rcp26  =t2m_DJF_rcp26.tas\n",
    "\n",
    "t2m_DJF_rcp26= t2m_DJF_rcp26[:9114,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_dec_rcp26 = xr.open_dataset(readin_mpige+\"tas_Amon_MPI-ESM_rcp26_all_i2005p3_200601-209912_traintest_dec_lowlow_anomnorm.nc\")\n",
    "t2m_dec_rcp26  =t2m_dec_rcp26.tas\n",
    "\n",
    "t2m_dec_rcp26= t2m_dec_rcp26[:9114,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_jan_rcp26 = xr.open_dataset(readin_mpige+\"tas_Amon_MPI-ESM_rcp26_all_i2005p3_200601-209912_traintest_jan_lowlow_anomnorm.nc\")\n",
    "t2m_jan_rcp26  =t2m_jan_rcp26.tas\n",
    "t2m_jan_rcp26= t2m_jan_rcp26[:9114,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_feb_rcp26 = xr.open_dataset(readin_mpige+\"tas_Amon_MPI-ESM_rcp26_all_i2005p3_200601-209912_traintest_feb_lowlow_anomnorm.nc\")\n",
    "t2m_feb_rcp26  =t2m_feb_rcp26.tas\n",
    "t2m_feb_rcp26= t2m_feb_rcp26[:9114,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slp_DJF_rcp26 = xr.open_dataset(readin_mpige+\"psl_Amon_MPI-ESM_rcp26_all_i2005p3_200601-209912_traintest_DJF_lowlow_anomnorm.nc\")\n",
    "slp_DJF_rcp26  =slp_DJF_rcp26.psl\n",
    "slp_DJF_rcp26= slp_DJF_rcp26[:9114,:,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20CRv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_DJF_codaearly = xr.open_dataset(readin_data+\"air.2m.mon.mean_DJF_18511900_lowlow_anomnorm.nc\")\n",
    "t2m_DJF_codaearly  =t2m_DJF_codaearly.air\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slp_DJF_codaearly = xr.open_dataset(readin_data+\"prmsl.mon.mean_DJF_18511900_lowlow_anomnorm.nc\")\n",
    "slp_DJF_codaearly  =slp_DJF_codaearly.prmsl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_dec_codaearly = xr.open_dataset(readin_data+\"air.2m.mon.mean_18501899_dec_lowlow_anomnorm.nc\")\n",
    "t2m_dec_codaearly  =t2m_dec_codaearly.air\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_jan_codaearly = xr.open_dataset(readin_data+\"air.2m.mon.mean_18511900_jan_lowlow_anomnorm.nc\")\n",
    "t2m_jan_codaearly  =t2m_jan_codaearly.air\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_feb_codaearly = xr.open_dataset(readin_data+\"air.2m.mon.mean_18511900_feb_lowlow_anomnorm.nc\")\n",
    "t2m_feb_codaearly  =t2m_feb_codaearly.air\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_DJF_codalate = xr.open_dataset(readin_data+\"air.2m.mon.mean_DJF_19012015_lowlow_anomnorm.nc\")\n",
    "t2m_DJF_codalate  =t2m_DJF_codalate.air\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slp_DJF_codalate = xr.open_dataset(readin_data+\"prmsl.mon.mean_DJF_19012015_lowlow_anomnorm.nc\")\n",
    "slp_DJF_codalate  =slp_DJF_codalate.prmsl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_dec_codalate = xr.open_dataset(readin_data+\"air.2m.mon.mean_19002014_dec_lowlow_anomnorm.nc\")\n",
    "t2m_dec_codalate  =t2m_dec_codalate.air\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_jan_codalate = xr.open_dataset(readin_data+\"air.2m.mon.mean_19012015_jan_lowlow_anomnorm.nc\")\n",
    "t2m_jan_codalate  =t2m_jan_codalate.air\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_feb_codalate = xr.open_dataset(readin_data+\"air.2m.mon.mean_19012015_feb_lowlow_anomnorm.nc\")\n",
    "t2m_feb_codalate  =t2m_feb_codalate.air\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare fields for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we can stack the data to multiple channels or postprocess in any form we need to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sst_hist_nov=thetao_nov.reset_index(\"time\",drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sst_ctl_nov=thetao_nov_ctl.reset_index(\"time\",drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sst_rcp26_nov=thetao_nov_rcp26.reset_index(\"time\",drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sst_coda_20th_nov=thetao_nov_codalate.reset_index(\"time\",drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sst_coda_19th_nov=thetao_nov_codaearly.reset_index(\"time\",drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create empty containers to stack (not needed here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That way you can stack multiple channels (such as sep, oct, nov ssts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_stacked_hist = []\n",
    "sst_stacked_ctl = []\n",
    "sst_stacked_rcp26 = []\n",
    "sst_stacked_coda_20th= []\n",
    "sst_stacked_ericlate= []\n",
    "sst_stacked_ericearly= []\n",
    "sst_stacked_coda_19th= []\n",
    "sst_stacked_hist_valid = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sst_stacked_hist.append(sst_hist_nov)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sst_stacked_ctl.append(sst_ctl_nov)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sst_stacked_rcp26.append(sst_rcp26_nov)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sst_stacked_coda_20th.append(sst_coda_20th_nov)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_codaearly.append(ds_codaearly)\n",
    "#data_codaearly.append(dst_codaearly)\n",
    "sst_stacked_coda_19th.append(sst_coda_19th_nov)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_stacked_hist = xr.concat(sst_stacked_hist, 'level').transpose('time', 'lat', 'lon', 'level')\n",
    "sst_stacked_ctl = xr.concat(sst_stacked_ctl, 'level').transpose('time', 'lat', 'lon', 'level')\n",
    "sst_stacked_rcp26 = xr.concat(sst_stacked_rcp26, 'level').transpose('time', 'lat', 'lon', 'level')\n",
    "sst_stacked_coda_20th = xr.concat(sst_stacked_coda_20th, 'level').transpose('time', 'lat', 'lon', 'level')\n",
    "sst_stacked_coda_19th= xr.concat(sst_stacked_coda_19th, 'level').transpose('time', 'lat', 'lon', 'level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if target==\"t2m_DJF\":\n",
    "    target_hist=t2m_DJF\n",
    "    target_coda_20th=t2m_DJF_codalate\n",
    "\n",
    "    target_ctl=t2m_DJF_ctl\n",
    "    target_rcp26=t2m_DJF_rcp26\n",
    "    target_coda_19th=t2m_DJF_codaearly\n",
    "\n",
    "if target==\"t2m_dec\":\n",
    "    target_hist=t2m_dec\n",
    "    target_coda_20th=t2m_dec_codalate\n",
    "\n",
    "    target_ctl=t2m_dec_ctl\n",
    "    target_rcp26=t2m_dec_rcp26\n",
    "    target_coda_19th=t2m_dec_codaearly\n",
    "\n",
    "\n",
    "if target==\"t2m_jan\":\n",
    "    target_hist=t2m_jan\n",
    "    target_coda_20th=t2m_jan_codalate\n",
    "\n",
    "    target_ctl=t2m_jan_ctl\n",
    "    target_rcp26=t2m_jan_rcp26\n",
    "    target_coda_19th=t2m_jan_codaearly\n",
    "\n",
    "    target_ericearly=t2m_jan_ericearly    \n",
    "    \n",
    "if target==\"t2m_feb\":\n",
    "    target_hist=t2m_feb\n",
    "    target_coda_20th=t2m_feb_codalate\n",
    "\n",
    "    target_ctl=t2m_feb_ctl\n",
    "    target_rcp26=t2m_feb_rcp26\n",
    "    target_coda_19th=t2m_feb_codaearly\n",
    "\n",
    "    target_ericearly=t2m_feb_ericearly \n",
    "\n",
    "if target==\"slp_DJF\":\n",
    "    target_hist=slp_DJF\n",
    "    target_coda_20th=slp_DJF_codalate\n",
    "\n",
    "    target_ctl=slp_DJF_ctl\n",
    "    target_rcp26=slp_DJF_rcp26\n",
    "    target_coda_19th=slp_DJF_codaearly\n",
    "\n",
    "    target_ericearly=slp_DJF_ericearly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert NA fields into zeroes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_stacked_hist=sst_stacked_hist.fillna(0).values\n",
    "sst_stacked_coda_20th=sst_stacked_coda_20th.fillna(0).values\n",
    "\n",
    "sst_stacked_coda_19th=sst_stacked_coda_19th.fillna(0).values\n",
    "sst_stacked_ctl=sst_stacked_ctl.fillna(0).values\n",
    "sst_stacked_rcp26=sst_stacked_rcp26.fillna(0).values\n",
    "\n",
    "\n",
    "target_hist=target_hist.fillna(0).values\n",
    "target_coda_20th=target_coda_20th.fillna(0).values\n",
    "target_coda_19th=target_coda_19th.fillna(0).values\n",
    "target_ctl=target_ctl.fillna(0).values\n",
    "target_rcp26=target_rcp26.fillna(0).values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define X values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_hist_coda_19th_ctl_rcp26=np.concatenate((sst_stacked_hist,sst_stacked_coda_19th,sst_stacked_ctl,sst_stacked_rcp26),axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define Y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_hist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_coda_19th.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_hist_coda_19th_ctl_rcp26=np.concatenate((target_hist,target_coda_19th,target_ctl,target_rcp26),axis=0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_hist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_stacked_hist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_stacked_coda_20th.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hist_coda_19th_ctl_rcp26_shuffled,y_hist_coda_19th_ctl_rcp26_shuffled = shuffle(X_hist_coda_19th_ctl_rcp26,y_hist_coda_19th_ctl_rcp26)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hist_coda_19th_ctl_rcp26_shuffled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hist_coda_19th_ctl_rcp26_shuffled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nchannels=X_hist_coda_19th_ctl_rcp26_shuffled.shape[3]\n",
    "N_train=X_hist_coda_19th_ctl_rcp26_shuffled.shape[0]\n",
    "N_test=sst_stacked_coda_20th.shape[0]\n",
    "nlat=X_hist_coda_19th_ctl_rcp26_shuffled.shape[1]\n",
    "nlon=X_hist_coda_19th_ctl_rcp26_shuffled.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reshape data for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hist_coda_19th_ctl_rcp26_shuffled4reg=X_hist_coda_19th_ctl_rcp26_shuffled.reshape((N_train,nlat*nlon))\n",
    "y_hist_coda_19th_ctl_rcp26_shuffled4reg=y_hist_coda_19th_ctl_rcp26_shuffled.reshape((N_train,nlat*nlon))\n",
    "sst_stacked_coda_20th4reg=sst_stacked_coda_20th.reshape((N_test,nlat*nlon))\n",
    "sst_stacked_ericearly4reg=sst_stacked_ericearly.reshape((N_eric,nlat*nlon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hist_coda_19th_ctl_rcp26_shuffled4reg_train=X_hist_coda_19th_ctl_rcp26_shuffled4reg[:round(X_hist_coda_19th_ctl_rcp26_shuffled4reg.shape[0]*0.8),:]\n",
    "y_hist_coda_19th_ctl_rcp26_shuffled4reg_train=y_hist_coda_19th_ctl_rcp26_shuffled4reg[:round(y_hist_coda_19th_ctl_rcp26_shuffled4reg.shape[0]*0.8),:]\n",
    "X_hist_coda_19th_ctl_rcp26_shuffled4reg_valid=X_hist_coda_19th_ctl_rcp26_shuffled4reg[round(X_hist_coda_19th_ctl_rcp26_shuffled4reg.shape[0]*0.8):,:]\n",
    "y_hist_coda_19th_ctl_rcp26_shuffled4reg_valid=y_hist_coda_19th_ctl_rcp26_shuffled4reg[round(y_hist_coda_19th_ctl_rcp26_shuffled4reg.shape[0]*0.8):,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hist_coda_19th_ctl_rcp26_shuffled4reg_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "regr=linear_model.LinearRegression(n_jobs=1)\n",
    "trained_regr=regr.fit(X_hist_coda_19th_ctl_rcp26_shuffled4reg_train,y_hist_coda_19th_ctl_rcp26_shuffled4reg_train)\n",
    "\n",
    "prediction_valid=trained_regr.predict(X_hist_coda_19th_ctl_rcp26_shuffled4reg_valid)\n",
    "prediction_valid_2d=prediction_valid.reshape(X_hist_coda_19th_ctl_rcp26_shuffled4reg_valid.shape[0],nlat,nlon)\n",
    "\n",
    "prediction=trained_regr.predict(sst_stacked_coda_20th4reg)\n",
    "prediction_2d=prediction.reshape(N_test,nlat,nlon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_4corr=y_hist_coda_19th_ctl_rcp26_shuffled4reg_valid.reshape(X_hist_coda_19th_ctl_rcp26_shuffled4reg_valid.shape[0],nlat*nlon)\n",
    "structure_dummy= np.arange(y_valid_4corr.shape[1], dtype=float)\n",
    "structure_dummy.shape\n",
    "for a in range(1,y_valid_4corr.shape[1]):\n",
    "    one_R=np.corrcoef(prediction_valid[:,a], y_valid_4corr[:,a])\n",
    "    structure_dummy[a]=one_R[0,1]\n",
    "corr_matrix=structure_dummy.reshape(nlat,nlon)\n",
    "corr_matrix.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_4corr=target_coda_20th.reshape(N_test,nlat*nlon)\n",
    "structure_dummy= np.arange(y_test_4corr.shape[1], dtype=float)\n",
    "structure_dummy.shape\n",
    "for a in range(1,y_test_4corr.shape[1]):\n",
    "    one_R=np.corrcoef(prediction[:,a], y_test_4corr[:,a])\n",
    "    structure_dummy[a]=one_R[0,1]\n",
    "corr_matrix=structure_dummy.reshape(nlat,nlon)\n",
    "corr_matrix.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define deep learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeriodicPadding2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, pad_width, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pad_width = pad_width\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if self.pad_width == 0:\n",
    "            return inputs\n",
    "        inputs_padded = tf.concat(\n",
    "            [inputs[:, :, -self.pad_width:, :], inputs, inputs[:, :, :self.pad_width, :]], axis=2)\n",
    "        # Zero padding in the lat direction\n",
    "        inputs_padded = tf.pad(inputs_padded, [[0, 0], [self.pad_width, self.pad_width], [0, 0], [0, 0]])\n",
    "        return inputs_padded\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'pad_width': self.pad_width})\n",
    "        return config\n",
    "\n",
    "\n",
    "class PeriodicConv2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters,\n",
    "                 kernel_size,\n",
    "                 conv_kwargs={},\n",
    "                 **kwargs, ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv_kwargs = conv_kwargs\n",
    "        if type(kernel_size) is not int:\n",
    "            assert kernel_size[0] == kernel_size[1], 'PeriodicConv2D only works for square kernels'\n",
    "            kernel_size = kernel_size[0]\n",
    "        pad_width = (kernel_size - 1) // 2\n",
    "        self.padding = PeriodicPadding2D(pad_width)\n",
    "        self.conv = Conv2D(\n",
    "            filters, kernel_size, padding='valid', **conv_kwargs\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.conv(self.padding(inputs))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'filters': self.filters, 'kernel_size': self.kernel_size, 'conv_kwargs': self.conv_kwargs})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(filters, kernels, input_shape, dr=0):\n",
    "    \"\"\"Fully convolutional network\"\"\"\n",
    "    x = input = Input(shape=input_shape)\n",
    "    for f, k in zip(filters[:-1], kernels[:-1]):\n",
    "        x = PeriodicConv2D(f, k)(x)\n",
    "        x = LeakyReLU()(x)\n",
    "        if dr > 0: x = Dropout(dr)(x)\n",
    "    output = PeriodicConv2D(filters[-1], kernels[-1])(x)\n",
    "    return keras.models.Model(input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_pool(filters, kernels, input_shape, dr=0):\n",
    "    \"\"\"Fully convolutional network\"\"\"\n",
    "    x = input = Input(shape=input_shape)\n",
    "    for f, k in zip(filters[:-1], kernels[:-1]):\n",
    "        x = PeriodicConv2D(f, k)(x)\n",
    "        x = LeakyReLU()(x)\n",
    "        x = MaxPooling2D((2, 2))(x)\n",
    "        if dr > 0: x = Dropout(dr)(x)\n",
    "    output = PeriodicConv2D(filters[-1], kernels[-1])(x)\n",
    "    return keras.models.Model(input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unet(input_shape, kernels,start_filters):\n",
    "    x = input = Input(shape=input_shape)\n",
    "    conv1 = PeriodicConv2D(filters=start_filters* 1, kernel_size=kernels)(x)\n",
    "    conv1 = LeakyReLU()(conv1)\n",
    "    pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "    pool1 = Dropout(0.5)(pool1)\n",
    "\n",
    "    conv2 = PeriodicConv2D(filters=start_filters* 2, kernel_size=kernels)(pool1)\n",
    "    conv2 = LeakyReLU()(conv2)\n",
    "    pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "    pool2 = Dropout(0.5)(pool2)\n",
    "\n",
    "    conv3 = PeriodicConv2D(filters=start_filters* 3, kernel_size=kernels)(pool2)\n",
    "    conv3 = LeakyReLU()(conv3)\n",
    "    pool3 = MaxPooling2D((2, 2))(conv3)\n",
    "    pool3 = Dropout(0.5)(pool3)\n",
    "\n",
    "    conv4 = PeriodicConv2D(filters=start_filters* 4, kernel_size=kernels)(pool3)\n",
    "    conv4 = LeakyReLU()(conv4)\n",
    "    pool4 = MaxPooling2D((2, 2))(conv4)\n",
    "    pool4 = Dropout(0.5)(pool4)\n",
    "\n",
    "    convm = Flatten()(pool4)\n",
    "    convm = Dense(288,activation=\"relu\")(convm)\n",
    "    convm = Dropout(0.3)(convm)\n",
    "    convm = Dense(3*6*64,activation=\"relu\")(convm)\n",
    "    convm = Dropout(0.3)(convm)\n",
    "\n",
    "\n",
    "    t4 = tf.reshape(convm, shape=[-1, 3, 6, 64])\n",
    "    deconv4 = tf.image.resize(t4,conv4.shape[1:3],method=resize_method)   \n",
    "    uconv4 = concatenate([deconv4, pool3])\n",
    "    uconv4 = PeriodicConv2D(filters=start_filters* 3, kernel_size=kernels)(uconv4)\n",
    "    uconv4 = LeakyReLU()(uconv4)\n",
    "\n",
    "    deconv3 = tf.image.resize(uconv4,conv3.shape[1:3],method=resize_method) \n",
    "    uconv3 = concatenate([deconv3, pool2])\n",
    "    uconv3 = PeriodicConv2D(filters=start_filters * 2, kernel_size=kernels)(uconv3)\n",
    "    uconv3 = LeakyReLU()(uconv3)\n",
    "    \n",
    "    deconv2 = tf.image.resize(uconv3,conv2.shape[1:3],method=resize_method)   \n",
    "    uconv2 = concatenate([deconv2, pool1])\n",
    "    uconv2 = PeriodicConv2D(filters=start_filters * 1, kernel_size=kernels)(uconv2)\n",
    "    uconv2 = LeakyReLU()(uconv2)\n",
    "    \n",
    "    deconv1 = tf.image.resize(uconv2,conv1.shape[1:3],method=resize_method)   \n",
    "    uconv1 = concatenate([deconv1, x])\n",
    "    output_layer= PeriodicConv2D(filters=1, kernel_size=kernels)(uconv1)\n",
    "    output = LeakyReLU()(output_layer)\n",
    "\n",
    "    return keras.models.Model(input, output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn = build_cnn([64, 64, 64, 64, 1], [5, 5, 5, 5, 5], (48, 96, 2),dr=0.1)\n",
    "cnn1 = build_cnn([64, 64, 64, 64, 1], [5, 5, 5, 5, 5], (nlat, nlon, nchannels),dr=0.1)\n",
    "cnn1x2 = build_cnn([64, 64, 64, 64, 1], [5, 5, 5, 5, 5], (nlat, nlon, nchannels),dr=0.1)\n",
    "\n",
    "cnn11 = build_cnn([64, 64, 64, 64, 1], [5, 5, 5, 5, 5], (nlat, nlon, nchannels),dr=0.1)\n",
    "cnn11_pool = build_cnn_pool([64, 64, 64, 64, 1], [5, 5, 5, 5, 5], (nlat, nlon, nchannels),dr=0.4)\n",
    "\n",
    "cnn11xxx = build_cnn([32, 64, 1], [5, 5, 5], (nlat, nlon, nchannels),dr=0.4)\n",
    "\n",
    "cnn11kiri= build_cnn([128, 8, 1], [5, 5, 5], (nlat, nlon, nchannels),dr=CNN11kiri_dic[target])# for t2m_DJF dr=0.1, for slp_DJF\n",
    "\n",
    "\n",
    "cnn44 = build_cnn([32, 64, 128, 256, 1], [5, 5, 5, 5, 5], (nlat, nlon, nchannels),dr=CNN44_dic[target]) # for t2m_DJF dr=0.2, for slp_DJF dr=0.4\n",
    "cnn44_pool = build_cnn_pool([32, 64, 128, 256, 1], [5, 5, 5, 5, 1], (nlat, nlon, 2),dr=0.4)\n",
    "\n",
    "cnn44_acc = build_cnn([32, 64, 128, 256, 1], [5, 5, 5, 5, 5], (nlat, nlon, nchannels),dr=0.4)\n",
    "\n",
    "\n",
    "\n",
    "cnn5 = build_cnn([64, 64, 64, 64, 1], [3, 3, 3, 3, 3], (nlat, nlon, nchannels),dr=0.1)\n",
    "cnn55 = build_cnn([64, 64, 64, 64, 1], [3, 3, 3, 3, 3], (nlat, nlon, nchannels),dr=0.4)\n",
    "#cnn66 = build_cnn([64, 64, 64, 64, 1], [5, 5, 5, 5, 5], (nlat, nlon, nchannels),dr=0.4)\n",
    "\n",
    "cnn7 = build_cnn([64, 64, 1], [5, 5, 5], (nlat, nlon, nchannels),dr=CNN7_dic[target])# for t2m_DJF dr=0.1, for slp_DJF dr=0.2\n",
    "cnn8 = build_cnn([64, 64, 1], [3, 3, 3], (nlat, nlon, nchannels),dr=CNN8_dic[target])# for t2m_DJF dr=0.1, for slp_DJF dr=0.2\n",
    "\n",
    "unet = build_unet(input_shape=(nlat, nlon, nchannels),kernels=3, start_filters=16)\n",
    "unet1 = build_unet(input_shape=(nlat, nlon, nchannels),kernels=5, start_filters=16)\n",
    "unet2 = build_unet(input_shape=(nlat, nlon, nchannels),kernels=3, start_filters=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unet_small(input_shape, kernels,start_filters):\n",
    "    x = input = Input(shape=input_shape)\n",
    "    conv1 = PeriodicConv2D(filters=start_filters* 1, kernel_size=kernels)(x)\n",
    "    conv1 = LeakyReLU()(conv1)\n",
    "    pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "    pool1 = Dropout(0.6)(pool1)\n",
    "\n",
    "    conv2 = PeriodicConv2D(filters=start_filters* 2, kernel_size=kernels)(pool1)\n",
    "    conv2 = LeakyReLU()(conv2)\n",
    "    pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "    pool2 = Dropout(0.6)(pool2)\n",
    "    \n",
    "    convm = Flatten()(pool2)\n",
    "    convm = Dense(256,activation=\"relu\")(convm)\n",
    "    convm = Dropout(0.5)(convm)\n",
    "    convm = Dense(1*2*64,activation=\"relu\")(convm)\n",
    "    convm = Dropout(0.5)(convm)\n",
    "\n",
    "\n",
    "    t4 = tf.reshape(convm, shape=[-1, 1, 2, 64])\n",
    "    \n",
    "    deconv2 = tf.image.resize(t4,conv2.shape[1:3],method=resize_method)   \n",
    "    uconv2 = concatenate([deconv2, pool1])\n",
    "    uconv2 = PeriodicConv2D(filters=start_filters * 1, kernel_size=kernels)(uconv2)\n",
    "    uconv2 = LeakyReLU()(uconv2)\n",
    "    \n",
    "    deconv1 = tf.image.resize(uconv2,conv1.shape[1:3],method=resize_method)   \n",
    "    uconv1 = concatenate([deconv1, x])\n",
    "    output_layer= PeriodicConv2D(filters=1, kernel_size=kernels)(uconv1)\n",
    "    output = LeakyReLU()(output_layer)\n",
    "\n",
    "    return keras.models.Model(input, output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = build_unet(input_shape=(nlat, nlon, nchannels),kernels=3, start_filters=16)\n",
    "unet1 = build_unet(input_shape=(nlat, nlon, nchannels),kernels=5, start_filters=16)\n",
    "unet2 = build_unet(input_shape=(nlat, nlon, nchannels),kernels=3, start_filters=32)\n",
    "\n",
    "\n",
    "unet2_small = build_unet_small(input_shape=(nlat, nlon, nchannels),kernels=3, start_filters=32)\n",
    "unet3_small = build_unet_small(input_shape=(nlat, nlon, nchannels),kernels=5, start_filters=32)\n",
    "unet4_small = build_unet_small(input_shape=(nlat, nlon, nchannels),kernels=5, start_filters=16)\n",
    "\n",
    "unet_small = build_unet_small(input_shape=(nlat, nlon, nchannels),kernels=3, start_filters=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hist_coda_19th_ctl_rcp26_shuffled4RNN=X_hist_coda_19th_ctl_rcp26_shuffled.reshape((N_train,nlat,nlon))\n",
    "y_hist_coda_19th_ctl_rcp26_shuffled4RNN=y_hist_coda_19th_ctl_rcp26_shuffled.reshape((N_train,nlat,nlon))\n",
    "sst_stacked_coda_20th4RNN=sst_stacked_coda_20th.reshape((N_test,nlat,nlon))\n",
    "sst_stacked_ericearly4RNN=sst_stacked_ericearly.reshape((N_eric,nlat,nlon))\n",
    "\n",
    "y_test_4RNN=target_coda_20th.reshape((N_test,nlat*nlon))\n",
    "structure_4RNN= np.arange(y_test_4RNN.shape[1], dtype=float)\n",
    "structure_4RNN.shape\n",
    "\n",
    "RNN1=Sequential()\n",
    "RNN1.add(LSTM(128,input_shape=(nlat,nlon),dropout=RNN1_dic[target],activation=\"tanh\",unroll=True))\n",
    "RNN1.add(Dense(nlat*nlon,activation=\"linear\"))\n",
    "RNN1.add(Reshape((nlat,nlon)))\n",
    "RNN1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "cnn1x2.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "\n",
    "\n",
    "RNN1.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "\n",
    "\n",
    "\n",
    "cnn11.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "cnn11xxx.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "\n",
    "cnn11kiri.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "\n",
    "\n",
    "cnn11_pool.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "\n",
    "cnn44.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "cnn44_acc.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"accuracy\"])\n",
    "\n",
    "cnn44_pool.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "\n",
    "cnn5.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "cnn55.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "cnn7.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "cnn8.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "#cnn66.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "unet1.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "unet2.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "unet_small.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "unet2_small.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "unet3_small.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n",
    "unet4_small.compile(keras.optimizers.Adam(1e-4), loss='mse',metrics=[\"mae\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the performance of RMSE and Correlation over the 20th century"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "also saving the predictions as netcdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_CNN11kiri_check=readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_CNN11kiri_m'+stat_model_member\n",
    "mc_CNN44_check=readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_CNN44_m'+stat_model_member\n",
    "mc_CNN7_check=readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_CNN7_m'+stat_model_member\n",
    "mc_CNN8_check=readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_CNN8_m'+stat_model_member\n",
    "mc_RNN1_check=readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_RNN1_m'+stat_model_member\n",
    "mc_unet_check=readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_unet_m'+stat_model_member\n",
    "mc_unet1_check=readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_unet1_m'+stat_model_member\n",
    "mc_unet2_check=readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_unet2_m'+stat_model_member\n",
    "mc_reg_check=readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_reg_m'+stat_model_member\n",
    "\n",
    "mc_unet_small_check = readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_unet1_small_m'+stat_model_member\n",
    "mc_unet2_small_check = readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_unet2_small_m'+stat_model_member\n",
    "mc_unet3_small_check = readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_unet3_small_m'+stat_model_member\n",
    "mc_unet4_small_check = readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_unet4_small_m'+stat_model_member\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn44.load_weights(mc_CNN44_check+\".h5\")\n",
    "RNN1.load_weights(mc_RNN1_check+\".h5\")\n",
    "\n",
    "cnn11kiri.load_weights(mc_CNN11kiri_check+'.h5')\n",
    "cnn8.load_weights(mc_CNN8_check+\".h5\")\n",
    "cnn7.load_weights(mc_CNN7_check+\".h5\")\n",
    "unet.load_weights(mc_unet_check+\".h5\")\n",
    "unet1.load_weights(mc_unet1_check+\".h5\")\n",
    "unet2.load_weights(mc_unet2_check+\".h5\")\n",
    "\n",
    "unet_small.load_weights(mc_unet_small_check+\".h5\")\n",
    "unet2_small.load_weights(mc_unet2_small_check+\".h5\")\n",
    "unet3_small.load_weights(mc_unet3_small_check+\".h5\")\n",
    "unet4_small.load_weights(mc_unet4_small_check+\".h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_list=[sst_stacked_coda_20th,sst_stacked_coda_20th,sst_stacked_coda_20th,sst_stacked_coda_20th,sst_stacked_coda_20th4RNN,sst_stacked_coda_20th,sst_stacked_coda_20th,sst_stacked_coda_20th,sst_stacked_coda_20th,sst_stacked_coda_20th,sst_stacked_coda_20th,sst_stacked_coda_20th]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list=[cnn11kiri,cnn44,cnn7,cnn8,RNN1,unet,unet1,unet2,unet_small,unet2_small,unet3_small,unet4_small]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# every model but regression\n",
    "for stat_model_member in stat_model_member_list:\n",
    "    mc_CNN11kiri_check=readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_CNN11kiri_m'+stat_model_member\n",
    "    mc_CNN44_check=readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_CNN44_m'+stat_model_member\n",
    "    mc_CNN7_check=readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_CNN7_m'+stat_model_member\n",
    "    mc_CNN8_check=readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_CNN8_m'+stat_model_member\n",
    "    mc_RNN1_check=readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_RNN1_m'+stat_model_member\n",
    "\n",
    "    mc_unet_check=readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_unet_m'+stat_model_member\n",
    "    mc_unet1_check=readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_unet1_m'+stat_model_member\n",
    "    mc_unet2_check=readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_unet2_m'+stat_model_member\n",
    "    mc_reg_check=readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_reg_m'+stat_model_member\n",
    "    mc_unet_small_check = readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_unet1_small_m'+stat_model_member\n",
    "    mc_unet2_small_check = readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_unet2_small_m'+stat_model_member\n",
    "    mc_unet3_small_check = readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_unet3_small_m'+stat_model_member\n",
    "    mc_unet4_small_check = readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_unet4_small_m'+stat_model_member\n",
    "\n",
    "    check_list=[mc_CNN11kiri_check,mc_CNN44_check,mc_CNN7_check,mc_CNN8_check,mc_RNN1_check,mc_unet_check,mc_unet1_check,mc_unet2_check,mc_unet_small_check,mc_unet2_small_check,mc_unet3_small_check,mc_unet4_small_check]\n",
    "\n",
    "    for i in range(len(model_list)):\n",
    "        corr_mean,netcdf=test_output(model=model_list[i],output_name=check_list[i],test_data=test_data_list[i],target=target_coda_20th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regresssion\n",
    "for member in stat_model_member_list:\n",
    "    print(\"member \"+member)\n",
    "    prediction_reg=create_regression_member(X_data=X_hist_coda_19th_ctl_rcp26,y_data=y_hist_coda_19th_ctl_rcp26,test_X_data=sst_stacked_coda_20th4reg,test_y_data=target_coda_20th)\n",
    "    mc_reg_check=readin_data+'best_modelanomnorm_mpicodaearlyctlrcp26_nov_sst_'+target+'_reg_m'+member\n",
    "    prediction_2d_reg=prediction_reg\n",
    "    get_corr(rec=prediction_2d_reg,original=target_coda_20th,folder=save_folder,name=mc_reg_check)\n",
    "    get_rmse(rec=prediction_2d_reg,original=target_coda_20th,folder=save_folder,name=mc_reg_check)\n",
    "    save_prediction_netcdf(prediction=prediction_2d_reg,name=mc_reg_check,folder=save_folder,lons=lons,lats=lats,var = var_name,units = var_unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imac311",
   "language": "python",
   "name": "imac311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
